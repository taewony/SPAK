---
title: "GPU Inference Mechanics & Ollama Setup"
status: completed
tags: ["infrastructure", "gpu", "ollama"]
difficulty: beginner
hardware_req: "NVIDIA GPU (min 4GB VRAM)"
tools: ["nvidia-smi", "ollama"]
---

# ğŸ–¥ï¸ GPU Inference & Ollama Setup

## 1. Goal
LLM ì¶”ë¡ (Inference) ê³¼ì •ì—ì„œ í•˜ë“œì›¨ì–´ ìì›ì´ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ ì´í•´í•˜ê³ , ë¡œì»¬ í™˜ê²½(Windows)ì—ì„œ Ollamaë¥¼ ìµœì í™”í•˜ì—¬ ì‹¤í–‰í•œë‹¤.

## 2. Key Concepts Learned

### Prefill (Processing)
* **ì •ì˜**: ì‚¬ìš©ìì˜ ì…ë ¥(Prompt)ì„ í† í°í™”í•˜ê³  KV Cacheë¥¼ ìƒì„±í•˜ëŠ” ë‹¨ê³„.
* **íŠ¹ì§•**: ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ì—¬ GPU ì—°ì‚° ëŠ¥ë ¥(Compute)ì— í¬ê²Œ ì˜ì¡´í•¨. ì§§ì€ ì‹œê°„ì— ê¸‰ê²©í•œ GPU ë¶€í•˜ ë°œìƒ.

### Decode (Generating)
* **ì •ì˜**: í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í°ì„ ìƒì„±í•˜ëŠ” ë‹¨ê³„.
* **íŠ¹ì§•**: ì´ì „ ìƒíƒœ(KV Cache)ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ë¶ˆëŸ¬ì™€ì•¼ í•˜ë¯€ë¡œ **ë©”ëª¨ë¦¬ ëŒ€ì—­í­(Memory Bandwidth)**ì´ ë³‘ëª©ì´ ë¨.

## 3. Observation Log
* **Command**: `nvidia-smi -l 1` (1ì´ˆë§ˆë‹¤ ê°±ì‹ )
* **Observation**:
    * Ollama ëª¨ë¸ ë¡œë“œ ì‹œ VRAM ì‚¬ìš©ëŸ‰ ê¸‰ì¦.
    * ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ ì‹œì‘ ì‹œ(Prefill) GPU Compute Usageê°€ ìˆœê°„ì ìœ¼ë¡œ íŠ€ì–´ì˜¤ë¦„.
    * ë‹µë³€ ìƒì„± ì¤‘(Decode)ì—ëŠ” VRAM ì‚¬ìš©ëŸ‰ì€ ìœ ì§€ë˜ë‚˜ Compute UsageëŠ” ë‚®ê²Œ ìœ ì§€ë¨.

## 4. Action Items
- [x] Install Ollama on Windows
- [x] Pull `gemma:2b` or `llama3` model
- [x] Run `nvidia-smi` monitor
- [x] Verify VRAM usage limits (Safe buffer ì„¤ì •)
