


행렬/벡터/텐서 등의 연산을 병렬로 고성능으로 처리하기 위한 계산 모델
Tile IR 및 DSL로 지원되는 병렬 프로그래밍 모델
Multi-core, 이기종 Computing 구조에서의 NUMA 나 Unified Memory 같은 메모리 모델
Tile을 vlaue sematic을 만족하는 기본 data type으로 하는 data model

Linux 위에 존재하는 C language system 및 C programming model과,
linux가 지원하는 thread 기반 병렬처리, 병행처리 모델

NUMA로 대표 되는 multi-core system에서 단일 memory space를 갖는 multi-thread programming
Unified Memory를 바탕으로 이기종 computing system에서 단일 memory space를 갖는 CPU-GPU kernel programming 모델

위와 같은 다양한 추상화 계층을 학문적으로 체계적으로 정리해 주고,
특히 linux 가 thread 기반 programming model 외에 Unified Memory를 가진 GB10 system에서 AI 가속기 상의 계산을 지원하기 위해 
Linux가 직접 programming model을 지원할 가능성에 대해 분석해줘.

최종적으로 HW system, linux, C/C++ language system, cuTile Python DSL model, tensor 나 tile 기반 계산 모델
등을 구체화된 실체들을 다시 한번 위에서 정립된 동일 체계 기준으로 설명해줘.

논문에 넣거나, 대학원 수업 내용을 만들고 싶어.


카테고리 이론(Category Theory):
  수학적 구조 간의 의미 보존적 매핑과 정형화에 매우 적합한 도구입니다.
  즉 상향/하향, 형식/비형식 사이의 의미 보존 관계를 다루는 이론적 기반을 제공합니다.

3계층 메타-시스템 구조
Level 2 — 인간 (Semantic Authority)
  인간은: 의미를 정의, “이게 맞다/아니다”를 결정, 시스템의 목적을 규정
  형식적으로:인간만이 모델 외부의 ‘판단자(judge)’

Level 1 — LLM (의미 변환자)
  LLM의 정확한 역할은: Semantic Transformer, 고수준 ↔ 저수준
         자연어 ↔ 수식, 수식 ↔ 코드, forward refinement / backward reasoning
  하지만: correctness를 보장하지는 않음, 의미를 보존하려 시도할 뿐
  즉: 의미 보존 후보를 생성하는 존재

Level 0 — 시스템 (System)
  코드, 커널, 하드웨어, 수식
  형식적 객체(Formal Object)
  실행과 상태 전이를 담당
  의미를 “가짐”이 아니라 “따름”

- 인간은 의미를 정의하고, LLM은 의미를 변환하며, 시스템은 의미를 실행한다.
| 역할  | 범주론적 대응               |
| --- | --------------------- |
| 시스템 | 객체(Object)들의 범주       |
| LLM | 구조를 보존하려는 함자(Functor) |
| 인간  | 함자의 선택/제약을 결정하는 외부 기준 |

인간의 의도 ↔ 실행 코드 사이에는:
  의미 간극, 표현 간극,  실행 제약 간극이 존재함. 이 간극은 단일 변환으로 메울 수 없습니다.
  상호 맵핑은 국소적으로만 성립합니다.

단계 분할의 수학적 의미
  범주론적으로 보면: 큰 함자 하나는 잘 정의되지 않음. 작은 함자들의 합성만 안정적

Tile 중심 병렬처리 계산모델에서
  알고리즘(수식으로 표현됨)을 고성능 Kernel 프로그램으로 변환하는 과정을
  Tile 이라는 Monoid 연산 단위로 정의하고, 수식을 Tile 단위로 분해하고, 다시 Tile 기반 DSL로 구현됨

LLM을 이용한 시스템 설계 및 구현은,
  인간이 시스템을 만드는 과정을 통제해서 최선으로 결과를 얻으려는 새로운 형태의 engineering이다.
(수학도 패턴 언어이고) 다양한 언어 패턴을 학습한 LLM은 수학적 추론 및 논증에 능하고, 
  따라서 인간의 의도나 목적을 최선은 결과물로 변환하는 과정을 위해
  문제공간을 적절한 'embedding subspace'로 분해하고, 이를 다시 solution space로 project해서 최선은 결과물을 얻을 수 있다.

AI 시대는 이미 학습된 LLM이 개입하기 때문에, 하향식 사고, 역산적 사고, 시스템적 사고가 필요하다.
  CS Engineering도 변해야 한다.

이러한 접근법을 'AI infused System Engineering'이라고 이름하자.

적용 사례 다음 3가지로 생각해보자.
여기서 우리는 '실험 가능한 단위', '검증 가능한 최소 단위' 혹은 'embedding subspace'를 도출 필요하다.
  1) 창업 아이디어 혹은 사업계획 가설을 web app으로 MVP로 빠르게 구현하여, 가설 검증하려 할 때,
  2) 게임 기획을 미리 구축된 LLM이 포함된 게임 엔진으로 기획이 정말 게임 재미를 유발할 수 있는지 테스트하려 할 때
  3) 오픈소스 포로젝트에서 문제 의식 및 이슈를 중심으로 지속적인 기능 추가와 시스템 진화가 가능하다.

이들에게서 'embedding subspace'를 무엇으로 정의해야 하는가?
   이 선택은 이것이 실행될 최종 abstract machine이 무엇인가에 제약을 받는다.

LLM은 문제를 “실행 가능한 의미 단위”로 바꾸는 기계
  LLM의 역할: 문제의 의미 구조를 명시화하고, 그 문제가 낳을 결과 공간을 먼저 펼쳐보는 것

문제 = “현재 상태와 기대 상태 사이의 의미적 불일치”

게임 기획은 의미적 규칙의 집합입니다.
  LLM이 다루기 좋은 최소 단위는: Rule + State + Consequence

이제 질문합니다:
LLM에서 post training 대신에, 그리고 RAG 및 vector DB를 사용하는 대신에,
LLM이 스스로 특정 domain에 대한 추론용 지식을 text로 누적하도록 만들 수 있을까?
그러니까 그 중간 수준 text를 LLM이 따로 작성하고 기억하는 보조 기억 장치처럼 사용하도록 하려면?

예를들어 게임 기획, 게임 스토리 구성, 게임 개발, 재미 효과와의 관계 등을 
고도의 추상적 semi-grammar 를 이용해 text 형태로 누적하면서, 베테랑 게임 개발자로 성장하는 방법은?

problem space에서 정의된 문제/이슈를
LLM이 'embedding subspace'로 분해하고, 이를 다시 solution space로 project해서 최선은 결과물을 얻도록 하려면,
어떻게 특정 domain의 지식이나 경험치를 누적 참조 가능하게 만들려면 어떻게?