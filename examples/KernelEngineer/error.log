python looplm/run_experiments.py

============================================================
ðŸš€ STARTING ADVANCED EXPERIMENT: G1_grokking_long
   Config: --n_embd=256 --n_head=4 --num_loops=16 --dropout=0.3 --max_iters=10000
   Output: looplm/experiments/G1_grokking_long
============================================================
[G1_grokking_long] Step 1: Training for 10000 iterations...
Running: python looplm/train_loop.py --n_embd=256 --n_head=4 --num_loops=16 --dropout=0.3 --max_iters=10000 --out_dir=experiments/G1_grokking_long --max_iters=10000
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: num_loops = 16
Overriding: dropout = 0.3
Overriding: max_iters = 10000
Overriding: out_dir = experiments/G1_grokking_long
Overriding: max_iters = 10000
Initializing from existing LoopLM checkpoint: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/out_addition/ckpt.pt
Vocab mismatch: Ckpt(12) vs Dataset(13).
Resetting lm_head and wte weights for new task.
Dimension mismatch or error loading state_dict: Error(s) in loading state_dict for LoopGPT:
        size mismatch for transformer.wpe.weight: copying a param with shape torch.Size([256, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
        size mismatch for transformer.h.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for transformer.h.attn.c_attn.weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([768, 256]).
        size mismatch for transformer.h.attn.c_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
        size mismatch for transformer.h.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for transformer.h.mlp.c_fc.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
        size mismatch for transformer.h.mlp.c_proj.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([256, 1024]).
        size mismatch for transformer.ln_f.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for step_embedding.weight: copying a param with shape torch.Size([12, 384]) from checkpoint, the shape in current model is torch.Size([16, 256]).
Starting from scratch instead.
Starting LoopLM Training on addition...
Config: 16 loops over 1 layer block
step 0: train loss 2.7258, val loss 2.7268, lr 0.0000e+00
iter 0: loss 2.7229, time 7004.48ms

iter 9990: loss 1.3230, time 49.82ms
LoopLM Training Complete. Checkpoint: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/experiments/G1_grokking_long/ckpt.pt

[G1_grokking_long] Step 2: Evaluating OOD performance (Generalization)...
Evaluating OOD for /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/experiments/G1_grokking_long/ckpt.pt (n=200, max_loops=None)...
OOD Accuracy: 0.50% (1/200)
âœ… [G1_grokking_long] Results: Accuracy 0.50%, Avg Steps: 15.98
[G1_grokking_long] Experiment completed and metrics indexed.
Traceback (most recent call last):
  File "/home/linux/taewony/SPAK/examples/KernelEngineer/looplm/run_experiments.py", line 137, in <module>
    main()
  File "/home/linux/taewony/SPAK/examples/KernelEngineer/looplm/run_experiments.py", line 115, in main
    with open(summary_path, "w") as f:
              ^^^^^^^^^^^^
NameError: name 'summary_path' is not defined
(env) linux@localhost:~/taewony/SPAK/examples/KernelEngineer$
