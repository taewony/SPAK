python nanoGPT/compare_implementations.py
number of parameters: 10.65M
--- Comparison: implementation vs implementation (Same Weights) ---
Max difference after Block 0: 0.000000e+00
[ORIGINAL model.py Output]:
 qzW3x'??BV$eygTt:MMl
KY&3X:NWW&dnLflFYEZzwF$B?UCg.wj,v
Hbb$JcVasPPsfU--SvviWPiRMMBdjdwyeQAVV:PjfUvj-
==================================================
[cuTile nanogpt_cutile.py Output]:
/pytorch/aten/src/ATen/native/cuda/TensorCompare.cu:112: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.
Traceback (most recent call last):
  File "/home/linux/taewony/SPAK/examples/KernelEngineer/nanoGPT/compare_implementations.py", line 75, in <module>
    y_cutile = model_cutile.generate(x, 100, temperature=0.8, top_k=200)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/linux/taewony/env/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/linux/taewony/SPAK/examples/KernelEngineer/nanoGPT/nanogpt_cutile.py", line 269, in generate
    logits, _ = self(idx_cond)
                ^^^^^^^^^^^^^^
  File "/home/linux/taewony/env/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/linux/taewony/env/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/linux/taewony/SPAK/examples/KernelEngineer/nanoGPT/nanogpt_cutile.py", line 261, in forward
    logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
                          ~^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: device-side assert triggered
Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
