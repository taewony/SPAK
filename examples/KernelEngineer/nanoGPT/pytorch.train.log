python train.py config/train_shakespeare_char.py
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

tokens per iteration will be: 16,384
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
/home/linux/taewony/SPAK/examples/KernelEngineer/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
W0220 09:21:28.253000 8900 torch/_inductor/utils.py:1613] [0/0] Not enough SMs to use max_autotune_gemm mode
step 0: train loss 4.2874, val loss 4.2823
iter 0: loss 4.2654, time 14084.21ms, mfu -100.00%
iter 10: loss 3.1461, time 27.61ms, mfu 13.50%
iter 20: loss 2.7310, time 27.71ms, mfu 13.49%
iter 30: loss 2.6176, time 27.81ms, mfu 13.48%
iter 40: loss 2.5754, time 27.89ms, mfu 13.47%
iter 50: loss 2.5250, time 27.66ms, mfu 13.47%
iter 60: loss 2.5141, time 27.67ms, mfu 13.47%
iter 70: loss 2.4947, time 27.85ms, mfu 13.46%
iter 80: loss 2.4933, time 27.66ms, mfu 13.46%
iter 90: loss 2.4705, time 27.86ms, mfu 13.45%
iter 100: loss 2.4708, time 27.85ms, mfu 13.45%
iter 110: loss 2.4593, time 27.87ms, mfu 13.44%
iter 120: loss 2.4269, time 27.71ms, mfu 13.44%
iter 130: loss 2.4120, time 27.95ms, mfu 13.43%
iter 140: loss 2.3972, time 27.88ms, mfu 13.42%
iter 150: loss 2.4086, time 27.87ms, mfu 13.42%
iter 160: loss 2.3692, time 27.73ms, mfu 13.42%
iter 170: loss 2.3555, time 27.85ms, mfu 13.41%
iter 180: loss 2.3243, time 27.90ms, mfu 13.41%
iter 190: loss 2.2529, time 27.89ms, mfu 13.40%
iter 200: loss 2.2107, time 27.92ms, mfu 13.40%
iter 210: loss 2.1394, time 27.87ms, mfu 13.40%
iter 220: loss 2.1441, time 27.91ms, mfu 13.39%
iter 230: loss 2.0687, time 27.89ms, mfu 13.39%
iter 240: loss 2.0801, time 27.88ms, mfu 13.39%
step 250: train loss 1.9718, val loss 2.0713
saving checkpoint to out-shakespeare-char
iter 250: loss 2.0380, time 3993.54ms, mfu 12.06%
iter 260: loss 1.9756, time 27.84ms, mfu 12.19%
iter 270: loss 1.9845, time 27.72ms, mfu 12.31%
iter 280: loss 1.9766, time 27.88ms, mfu 12.42%
iter 290: loss 1.9112, time 27.91ms, mfu 12.51%
iter 300: loss 1.8939, time 27.95ms, mfu 12.59%
iter 310: loss 1.8665, time 27.94ms, mfu 12.67%
iter 320: loss 1.8512, time 27.89ms, mfu 12.74%
iter 330: loss 1.8221, time 27.90ms, mfu 12.80%
iter 340: loss 1.7919, time 27.89ms, mfu 12.86%
iter 350: loss 1.8287, time 27.90ms, mfu 12.91%
iter 360: loss 1.7696, time 27.89ms, mfu 12.95%
iter 370: loss 1.7429, time 27.91ms, mfu 12.99%
iter 380: loss 1.7290, time 27.89ms, mfu 13.03%
iter 390: loss 1.7330, time 27.81ms, mfu 13.07%
iter 400: loss 1.7656, time 27.87ms, mfu 13.10%
iter 410: loss 1.7004, time 27.89ms, mfu 13.12%
iter 420: loss 1.7182, time 27.92ms, mfu 13.14%
iter 430: loss 1.6858, time 27.92ms, mfu 13.16%
iter 440: loss 1.6521, time 27.89ms, mfu 13.18%
iter 450: loss 1.6554, time 27.88ms, mfu 13.20%
iter 460: loss 1.6022, time 27.93ms, mfu 13.22%
iter 470: loss 1.6450, time 27.91ms, mfu 13.23%
iter 480: loss 1.6202, time 27.84ms, mfu 13.24%
iter 490: loss 1.6001, time 27.94ms, mfu 13.25%
step 500: train loss 1.5217, val loss 1.7147
saving checkpoint to out-shakespeare-char
iter 500: loss 1.5968, time 4007.82ms, mfu 11.94%
iter 510: loss 1.6123, time 27.77ms, mfu 12.09%
iter 520: loss 1.5911, time 27.91ms, mfu 12.21%
iter 530: loss 1.5596, time 27.94ms, mfu 12.32%
iter 540: loss 1.6150, time 27.88ms, mfu 12.43%
iter 550: loss 1.5592, time 28.37ms, mfu 12.50%
iter 560: loss 1.5602, time 27.93ms, mfu 12.58%
iter 570: loss 1.5642, time 27.73ms, mfu 12.67%
iter 580: loss 1.5268, time 27.92ms, mfu 12.74%
iter 590: loss 1.4943, time 28.01ms, mfu 12.79%
iter 600: loss 1.5153, time 27.94ms, mfu 12.85%
iter 610: loss 1.5372, time 27.94ms, mfu 12.90%
iter 620: loss 1.5316, time 27.91ms, mfu 12.94%
iter 630: loss 1.5092, time 27.92ms, mfu 12.98%
iter 640: loss 1.4618, time 27.99ms, mfu 13.02%
iter 650: loss 1.4919, time 27.92ms, mfu 13.05%
iter 660: loss 1.4967, time 27.94ms, mfu 13.08%
iter 670: loss 1.4377, time 27.91ms, mfu 13.10%
iter 680: loss 1.5034, time 27.91ms, mfu 13.13%
iter 690: loss 1.4662, time 27.97ms, mfu 13.15%
iter 700: loss 1.4768, time 27.89ms, mfu 13.17%
iter 710: loss 1.4505, time 27.88ms, mfu 13.19%
iter 720: loss 1.4383, time 27.90ms, mfu 13.21%
iter 730: loss 1.4154, time 28.02ms, mfu 13.21%
iter 740: loss 1.4279, time 28.00ms, mfu 13.22%
step 750: train loss 1.3560, val loss 1.5812
saving checkpoint to out-shakespeare-char
iter 750: loss 1.4163, time 3999.81ms, mfu 11.91%
iter 760: loss 1.4382, time 27.88ms, mfu 12.06%
iter 770: loss 1.4260, time 27.92ms, mfu 12.19%
iter 780: loss 1.4145, time 28.03ms, mfu 12.30%
iter 790: loss 1.4092, time 28.02ms, mfu 12.40%
iter 800: loss 1.4290, time 27.83ms, mfu 12.50%
iter 810: loss 1.3993, time 28.02ms, mfu 12.58%
iter 820: loss 1.4030, time 28.04ms, mfu 12.65%
iter 830: loss 1.3912, time 28.04ms, mfu 12.71%
iter 840: loss 1.3999, time 28.04ms, mfu 12.77%
iter 850: loss 1.3868, time 28.00ms, mfu 12.82%
iter 860: loss 1.3903, time 27.99ms, mfu 12.87%
iter 870: loss 1.3937, time 28.00ms, mfu 12.92%
iter 880: loss 1.3667, time 28.00ms, mfu 12.96%
iter 890: loss 1.3911, time 28.01ms, mfu 12.99%
iter 900: loss 1.3700, time 27.96ms, mfu 13.02%
iter 910: loss 1.3199, time 28.01ms, mfu 13.05%
iter 920: loss 1.3535, time 27.94ms, mfu 13.08%
iter 930: loss 1.3532, time 28.00ms, mfu 13.10%
iter 940: loss 1.3397, time 28.00ms, mfu 13.12%
iter 950: loss 1.3532, time 27.97ms, mfu 13.14%
iter 960: loss 1.3589, time 28.05ms, mfu 13.16%
iter 970: loss 1.3553, time 28.03ms, mfu 13.17%
iter 980: loss 1.3504, time 27.97ms, mfu 13.19%
iter 990: loss 1.3321, time 27.98ms, mfu 13.20%
step 1000: train loss 1.2715, val loss 1.5201
saving checkpoint to out-shakespeare-char
...
step 4750: train loss 0.6328, val loss 1.6914
iter 4750: loss 0.7995, time 3945.10ms, mfu 11.83%
iter 4760: loss 0.8165, time 28.09ms, mfu 11.98%
iter 4770: loss 0.7978, time 28.16ms, mfu 12.10%
iter 4780: loss 0.8115, time 28.17ms, mfu 12.22%
iter 4790: loss 0.8307, time 28.11ms, mfu 12.32%
iter 4800: loss 0.8151, time 28.14ms, mfu 12.41%
iter 4810: loss 0.8442, time 28.18ms, mfu 12.49%
iter 4820: loss 0.8181, time 28.07ms, mfu 12.57%
iter 4830: loss 0.8171, time 28.13ms, mfu 12.64%
iter 4840: loss 0.8289, time 28.14ms, mfu 12.70%
iter 4850: loss 0.8144, time 28.12ms, mfu 12.75%
iter 4860: loss 0.8138, time 28.14ms, mfu 12.80%
iter 4870: loss 0.8085, time 28.10ms, mfu 12.85%
iter 4880: loss 0.8247, time 28.12ms, mfu 12.89%
iter 4890: loss 0.8115, time 28.11ms, mfu 12.93%
iter 4900: loss 0.8042, time 28.11ms, mfu 12.96%
iter 4910: loss 0.8343, time 28.11ms, mfu 12.99%
iter 4920: loss 0.8187, time 28.16ms, mfu 13.01%
iter 4930: loss 0.8106, time 28.11ms, mfu 13.04%
iter 4940: loss 0.7983, time 28.18ms, mfu 13.06%
iter 4950: loss 0.8202, time 28.10ms, mfu 13.08%
iter 4960: loss 0.8280, time 28.12ms, mfu 13.09%
iter 4970: loss 0.7935, time 28.10ms, mfu 13.11%
iter 4980: loss 0.7928, time 28.13ms, mfu 13.12%
iter 4990: loss 0.8180, time 28.16ms, mfu 13.14%
step 5000: train loss 0.6173, val loss 1.7097
iter 5000: loss 0.8142, time 3951.21ms, mfu 11.83%
