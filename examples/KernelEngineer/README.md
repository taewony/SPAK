# SPAK: Semiformal DSL-based GPU Kernel Engineering

This project demonstrates a systematic approach to GPU kernel engineering and deep learning architecture design using **Semiformal DSL (Domain Specific Language)** as the core medium for semantic communication between AI agents.

## ğŸ¤– The Dual-Agent Paradigm

LLM agents operate in two distinct specialized roles, synchronized through the DSL:
1.  **System Engineer (Architect)**: Responsible for high-level design, DSL definition, and defining the "laws of physics" for the model.
2.  **Kernel Engineer (Implementer)**: Responsible for low-level GPU kernel implementation (cuTile/CUDA) and conducting error-free experiments.

---

## ğŸ† Final Research Results (The 12-Digit Frontier)

Our systematic evaluation on Out-of-Distribution (OOD) arithmetic tasks yields the following breakthrough results:

### **Experimental Setup & Zero-Shot Conditions**
To evaluate logical generalization, we use a curriculum-based training approach:
*   **Training Distribution**: Primarily 1-4 digit operands, augmented with **30% "Bridge Data" (5-6 digits)** to provide a signal for length scaling.
*   **Zero-Shot OOD Evaluation**: All performance metrics for 8, 10, and 12-digit addition are obtained in a **strictly zero-shot manner**, as the model never encountered these lengths during training.

### **Comparison Table**
... (ê¸°ì¡´ í…Œì´ë¸”) ...

### **Experimental Case Descriptions**
- **GPT-12L (Static)**: A standard Transformer model with 12 spatial layers. Used as the primary baseline to represent traditional fixed-depth architectures.
- **LoopLM-12 (Dynamic)**: A recurrent model using 1 shared layer repeated 12 times. Demonstrates that temporal depth is more efficient than spatial depth.
- **LoopLM-30 (Deep Thinking)**: A recurrent model trained with a larger recurrent limit (30 loops) to test the boundaries of algorithmic generalization on 8-digit addition.
- **LoopLM-128e (Efficient)**: An extremely compressed version with only 128 embedding dimensions, proving that recurrent logic requires significantly fewer parameters to outperform static giants.
- **LoopLM-12 (Test-Time 24)**: A robustness test where a model trained on 12 loops is forced to compute for 24 loops during inference, showcasing "Test-Time Compute" stability.

### **Visual Assets**
![Generalization Curve](looplm/paper_assets/fig1_generalization_curve.png)
*Figure 1: Accuracy drop-off as operand length increases. LoopLM variants maintain high performance where static models collapse.*

![Test-Time Compute](looplm/paper_assets/fig2_test_time_compute.png)
*Figure 2: Impact of increasing inference loops without additional training.*

---

## ğŸ‡°ğŸ‡· [í•œê¸€ ë²„ì „] ìµœì¢… ì—°êµ¬ ì„±ê³¼ ë° ìš”ì•½

### **ìµœì¢… ì„±ì í‘œ**
| ëª¨ë¸ ì•„í‚¤í…ì²˜ | 1-4ìë¦¬ (í•™ìŠµ) | 5-6ìë¦¬ (OOD) | 8ìë¦¬ (OOD) | íŒŒë¼ë¯¸í„° | íš¨ìœ¨ì„± |
| :--- | :---: | :---: | :---: | :---: | :---: |
| **GPT-12L (Static)** | 100% | 61.90% | 0.00% | ~85M | 1.0x |
| **LoopLM-12 (Dynamic)** | 100% | **80.00%** | 0.00% | **~7M** | **12.1x** |
| **LoopLM-30 (Deep)** | 100% | **95.24%** | **2.59%** | **~7M** | **12.1x** |
| **LoopLM-128e (Efficient)**| 100% | 76.19% | 0.00% | **~2M** | **42.5x** |
| **LoopLM-12 (Test-Time 24)**| 100% | 78.10% | 0.00% | **~7M** | **N/A** |

### **ì‹¤í—˜ ì¼€ì´ìŠ¤ë³„ ìƒì„¸ ì„¤ëª…**
- **GPT-12L (Static)**: 12ê°œì˜ ê³µê°„ì  ì¸µì„ ê°€ì§„ í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸. ì „í†µì ì¸ ê³ ì • ê¹Šì´ ì•„í‚¤í…ì²˜ë¥¼ ëŒ€í‘œí•˜ëŠ” ì£¼ìš” ëŒ€ì¡°êµ°ì…ë‹ˆë‹¤.
- **LoopLM-12 (Dynamic)**: 1ê°œì˜ ê³µìœ  ì¸µì„ 12ë²ˆ ë°˜ë³µí•˜ëŠ” ì¬ê·€ ëª¨ë¸. ì‹œê°„ì  ê¹Šì´ê°€ ê³µê°„ì  ê¹Šì´ë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì ì„ì„ ì¦ëª…í•©ë‹ˆë‹¤.
- **LoopLM-30 (Deep Thinking)**: 8ìë¦¬ ë§ì…ˆì˜ ì•Œê³ ë¦¬ì¦˜ ì¼ë°˜í™” í•œê³„ë¥¼ ì‹œí—˜í•˜ê¸° ìœ„í•´ 30íšŒ ë£¨í”„ë¡œ í•™ìŠµëœ ì¬ê·€ ëª¨ë¸ì…ë‹ˆë‹¤.
- **LoopLM-128e (Efficient)**: ì— ë² ë”© ì°¨ì›ì„ 128ë¡œ ì¤„ì¸ ì´ˆì••ì¶• ë²„ì „. ì¬ê·€ì  ë…¼ë¦¬ê°€ ê±°ëŒ€ ì •ì  ëª¨ë¸ì„ ì••ë„í•˜ëŠ” ë° ë§¤ìš° ì ì€ íŒŒë¼ë¯¸í„°ë§Œ í•„ìš”í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.
- **LoopLM-12 (Test-Time 24)**: 12ë£¨í”„ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ì¶”ë¡  ì‹œì—ë§Œ 24ë£¨í”„ë¡œ í™•ì¥í•˜ì—¬ ì‹¤í–‰í•œ ê°•ê±´ì„± í…ŒìŠ¤íŠ¸. "ì¶”ë¡  ì‹œ ì—°ì‚°ëŸ‰ í™•ì¥(Test-Time Compute)"ì˜ ì•ˆì •ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

## ğŸ›  Tech Stack
*   **Language**: Python, PyTorch
*   **Kernel**: CUDA, **cuTile** (SPAK-native GPU abstraction)
*   **Architecture**: Blackwell-ready (RTX 5070)
*   **Orchestration**: Semiformal DSL + LLM Agents (Gemini PRO, Gemini CLI)
