# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# SPDX-License-Identifier: MIT

import math

import pytest
import torch

from tilegym.backend import set_backend
from tilegym.ops.cutile.attention import FlashAttentionFunction
from tilegym.ops.cutile.attention import fmha_backward
from tilegym.ops.cutile.attention import fmha_forward_with_lse
from tilegym.ops.cutile.attention import tile_fmha_functional
from tilegym.ops.cutile.attention import tile_fmha_with_backward

from .. import common


def get_data(*shape, dtype, device, requires_grad=False, mean=0.0, std=0.5):
    """Generate random test data."""
    out = torch.empty(*shape, dtype=dtype, device=device).normal_(mean, std)
    if requires_grad:
        out.requires_grad_(True)
    return out


def reference_attention(q, k, v, scaling=None, is_causal=False):
    """PyTorch reference implementation using scaled_dot_product_attention."""
    if scaling is None:
        scaling = 1.0 / math.sqrt(q.size(-1))

    return torch.nn.functional.scaled_dot_product_attention(
        q, k, v, attn_mask=None, dropout_p=0.0, is_causal=is_causal, scale=scaling
    )


def reference_backward(q, k, v, do, scaling=None, is_causal=False):
    """Compute reference gradients using PyTorch autograd."""
    q = q.clone().detach().requires_grad_(True)
    k = k.clone().detach().requires_grad_(True)
    v = v.clone().detach().requires_grad_(True)

    if scaling is None:
        scaling = 1.0 / math.sqrt(q.size(-1))

    out = torch.nn.functional.scaled_dot_product_attention(
        q, k, v, attn_mask=None, dropout_p=0.0, is_causal=is_causal, scale=scaling
    )
    out.backward(do)

    return q.grad.clone(), k.grad.clone(), v.grad.clone()


class Test_FMHA_Backward(common.PyTestCase):
    """Test suite for Flash Attention backward pass."""

    _backends = ["cutile"]

    # Regular shapes (power of 2)
    @pytest.mark.parametrize(
        "batch_size, num_heads, seq_len, head_dim, is_causal, dtype",
        [
            # Small shapes
            # (1, 1, 64, 64, False, torch.float16),
            # (1, 1, 64, 64, True, torch.float16),
            # (1, 4, 128, 64, True, torch.float16),
            (2, 8, 256, 64, True, torch.float16),
            # Medium shapes
            # (1, 8, 512, 128, True, torch.float16),
            # (2, 16, 1024, 128, True, torch.float16),
            # bfloat16
            # (1, 8, 256, 128, True, torch.bfloat16),
            # (2, 8, 512, 64, False, torch.bfloat16),
            # Non-causal
            # (1, 4, 256, 64, False, torch.float16),
            (2, 8, 512, 128, False, torch.bfloat16),
        ],
    )
    @pytest.mark.parametrize("backend", _backends)
    def test_op_backward_regular_shapes(
        self,
        batch_size,
        num_heads,
        seq_len,
        head_dim,
        is_causal,
        dtype,
        backend,
        arch,
    ):
        """Test backward pass with regular (power of 2) shapes."""
        try:
            set_backend(backend)
        except Exception as e:
            pytest.skip(f"Backend is not supported: {e}")

        self.setUp()

        # Create test tensors
        q = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)
        k = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)
        v = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)

        sm_scale = 1.0 / math.sqrt(head_dim)

        # Forward pass
        out = tile_fmha_with_backward(q, k, v, scaling=sm_scale, is_causal=is_causal)

        # Random gradient for backward
        do = torch.randn_like(out)

        # Backward pass
        out.backward(do)
        dq_cutile = q.grad.clone()
        dk_cutile = k.grad.clone()
        dv_cutile = v.grad.clone()

        # Reset gradients
        q.grad = None
        k.grad = None
        v.grad = None

        # Reference backward
        dq_ref, dk_ref, dv_ref = reference_backward(
            q.detach(), k.detach(), v.detach(), do, scaling=sm_scale, is_causal=is_causal
        )

        # Check correctness
        atol = 5e-2
        rtol = 1e-2

        torch.testing.assert_close(dq_cutile, dq_ref, atol=atol, rtol=rtol)
        torch.testing.assert_close(dk_cutile, dk_ref, atol=atol, rtol=rtol)
        torch.testing.assert_close(dv_cutile, dv_ref, atol=atol, rtol=rtol)

    # Irregular shapes (non-power of 2)
    @pytest.mark.parametrize(
        "batch_size, num_heads, seq_len, head_dim, is_causal, dtype",
        [
            # Odd sequence lengths
            (1, 4, 127, 64, True, torch.float16),
            # (1, 4, 129, 64, True, torch.float16),
            # (1, 8, 255, 128, True, torch.float16),
            # (2, 8, 257, 128, True, torch.float16),
            # Prime sequence lengths
            # (1, 4, 131, 64, True, torch.float16),
            # (1, 4, 251, 64, True, torch.bfloat16),
            # Large irregular
            # (1, 8, 1023, 128, True, torch.float16),
            # (1, 8, 1025, 128, True, torch.float16),
            (2, 8, 2047, 128, True, torch.bfloat16),
            # (2, 8, 2049, 128, True, torch.bfloat16),
        ],
    )
    @pytest.mark.parametrize("backend", _backends)
    def test_op_backward_irregular_shapes(
        self,
        batch_size,
        num_heads,
        seq_len,
        head_dim,
        is_causal,
        dtype,
        backend,
        arch,
    ):
        """Test backward pass with irregular (non-power of 2) shapes."""
        try:
            set_backend(backend)
        except Exception as e:
            pytest.skip(f"Backend is not supported: {e}")

        self.setUp()

        q = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)
        k = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)
        v = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)

        sm_scale = 1.0 / math.sqrt(head_dim)

        out = tile_fmha_with_backward(q, k, v, scaling=sm_scale, is_causal=is_causal)
        do = torch.randn_like(out)
        out.backward(do)

        dq_cutile = q.grad.clone()
        dk_cutile = k.grad.clone()
        dv_cutile = v.grad.clone()

        q.grad = None
        k.grad = None
        v.grad = None

        dq_ref, dk_ref, dv_ref = reference_backward(
            q.detach(), k.detach(), v.detach(), do, scaling=sm_scale, is_causal=is_causal
        )

        atol = 5e-2
        rtol = 1e-2

        torch.testing.assert_close(dq_cutile, dq_ref, atol=atol, rtol=rtol)
        torch.testing.assert_close(dk_cutile, dk_ref, atol=atol, rtol=rtol)
        torch.testing.assert_close(dv_cutile, dv_ref, atol=atol, rtol=rtol)

    # Corner cases
    @pytest.mark.parametrize(
        "batch_size, num_heads, seq_len, head_dim, is_causal, dtype",
        [
            # Single element batch
            (1, 1, 64, 64, True, torch.float16),
            # Single head
            # (2, 1, 128, 64, True, torch.float16),
            # Large batch
            # (8, 4, 128, 64, True, torch.float16),
            # Many heads
            (1, 32, 128, 64, True, torch.float16),
            # Small head dim
            # (2, 4, 128, 32, True, torch.float16),
            # Large head dim
            # (1, 4, 64, 128, True, torch.float16),
            # Very short sequence
            # (2, 4, 16, 64, True, torch.float16),
            # (2, 4, 32, 64, True, torch.float16),
        ],
    )
    @pytest.mark.parametrize("backend", _backends)
    def test_op_backward_corner_cases(
        self,
        batch_size,
        num_heads,
        seq_len,
        head_dim,
        is_causal,
        dtype,
        backend,
        arch,
    ):
        """Test backward pass with corner case configurations."""
        try:
            set_backend(backend)
        except Exception as e:
            pytest.skip(f"Backend is not supported: {e}")

        self.setUp()

        q = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)
        k = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)
        v = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)

        sm_scale = 1.0 / math.sqrt(head_dim)

        out = tile_fmha_with_backward(q, k, v, scaling=sm_scale, is_causal=is_causal)
        do = torch.randn_like(out)
        out.backward(do)

        dq_cutile = q.grad.clone()
        dk_cutile = k.grad.clone()
        dv_cutile = v.grad.clone()

        q.grad = None
        k.grad = None
        v.grad = None

        dq_ref, dk_ref, dv_ref = reference_backward(
            q.detach(), k.detach(), v.detach(), do, scaling=sm_scale, is_causal=is_causal
        )

        atol = 5e-2
        rtol = 1e-2

        torch.testing.assert_close(dq_cutile, dq_ref, atol=atol, rtol=rtol)
        torch.testing.assert_close(dk_cutile, dk_ref, atol=atol, rtol=rtol)
        torch.testing.assert_close(dv_cutile, dv_ref, atol=atol, rtol=rtol)

    # Test forward output consistency
    @pytest.mark.parametrize(
        "batch_size, num_heads, seq_len, head_dim, is_causal, dtype",
        [
            (1, 8, 256, 128, True, torch.float16),
            (2, 8, 512, 64, False, torch.bfloat16),
        ],
    )
    @pytest.mark.parametrize("backend", _backends)
    def test_op_forward_with_lse_matches_reference(
        self,
        batch_size,
        num_heads,
        seq_len,
        head_dim,
        is_causal,
        dtype,
        backend,
        arch,
    ):
        """Verify forward pass with LSE produces correct output."""
        try:
            set_backend(backend)
        except Exception as e:
            pytest.skip(f"Backend is not supported: {e}")

        self.setUp()

        q = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda")
        k = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda")
        v = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda")

        sm_scale = 1.0 / math.sqrt(head_dim)

        # Forward with LSE
        out_cutile, lse = fmha_forward_with_lse(q, k, v, sm_scale, is_causal)

        # Reference
        out_ref = reference_attention(q, k, v, scaling=sm_scale, is_causal=is_causal)

        atol = 5e-2
        rtol = 1e-2

        torch.testing.assert_close(out_cutile, out_ref, atol=atol, rtol=rtol)

    # Test functional API mode selection
    @pytest.mark.parametrize("backend", _backends)
    def test_op_functional_api_inference_mode(self, backend, arch):
        """Test that functional API correctly uses inference mode when appropriate."""
        try:
            set_backend(backend)
        except Exception as e:
            pytest.skip(f"Backend is not supported: {e}")

        q = get_data(1, 4, 128, 64, dtype=torch.float16, device="cuda", requires_grad=False)
        k = get_data(1, 4, 128, 64, dtype=torch.float16, device="cuda", requires_grad=False)
        v = get_data(1, 4, 128, 64, dtype=torch.float16, device="cuda", requires_grad=False)

        # Test with no_grad
        with torch.no_grad():
            out = tile_fmha_functional(q, k, v, is_causal=True)
            assert out is not None

        # Test with inference_mode
        with torch.inference_mode():
            out = tile_fmha_functional(q, k, v, is_causal=True)
            assert out is not None

    @pytest.mark.parametrize("backend", _backends)
    def test_op_functional_api_training_mode(self, backend, arch):
        """Test that functional API correctly uses training mode with gradients."""
        try:
            set_backend(backend)
        except Exception as e:
            pytest.skip(f"Backend is not supported: {e}")

        q = get_data(1, 4, 128, 64, dtype=torch.float16, device="cuda", requires_grad=True)
        k = get_data(1, 4, 128, 64, dtype=torch.float16, device="cuda", requires_grad=True)
        v = get_data(1, 4, 128, 64, dtype=torch.float16, device="cuda", requires_grad=True)

        # Forward
        out = tile_fmha_functional(q, k, v, is_causal=True)

        # Backward should work
        do = torch.randn_like(out)
        out.backward(do)

        assert q.grad is not None
        assert k.grad is not None
        assert v.grad is not None


class Test_FMHA_Backward_GQA(common.PyTestCase):
    """Test suite for Flash Attention backward with Grouped Query Attention."""

    _backends = ["cutile"]

    @pytest.mark.parametrize(
        "batch_size, num_heads, num_kv_heads, seq_len, head_dim, is_causal, dtype",
        [
            # Simple GQA (2:1)
            (1, 8, 4, 128, 64, True, torch.float16),
            # (2, 8, 4, 256, 64, True, torch.float16),
            # GQA (4:1)
            # (1, 16, 4, 128, 64, True, torch.float16),
            # (1, 32, 8, 256, 64, True, torch.bfloat16),
            # Multi-Query Attention (all Q heads share 1 KV head)
            (1, 8, 1, 128, 64, True, torch.float16),
            # (2, 16, 1, 256, 64, True, torch.float16),
            # GQA with irregular sequence lengths
            # (1, 8, 4, 127, 64, True, torch.float16),
            # (1, 8, 2, 255, 64, True, torch.float16),
            # (2, 16, 4, 257, 128, True, torch.bfloat16),
            # GQA non-causal
            # (1, 8, 4, 128, 64, False, torch.float16),
            # (2, 16, 4, 256, 128, False, torch.bfloat16),
            # GQA with larger head_dim
            # (1, 8, 2, 128, 128, True, torch.float16),
            # Llama-style ratios (8:1)
            # (1, 32, 4, 128, 64, True, torch.float16),
        ],
    )
    @pytest.mark.parametrize("backend", _backends)
    def test_op_backward_gqa(
        self,
        batch_size,
        num_heads,
        num_kv_heads,
        seq_len,
        head_dim,
        is_causal,
        dtype,
        backend,
        arch,
    ):
        """Test backward pass with Grouped Query Attention (GQA)."""
        try:
            set_backend(backend)
        except Exception as e:
            pytest.skip(f"Backend is not supported: {e}")

        self.setUp()

        # Q has more heads than K, V
        q = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)
        k = get_data(batch_size, num_kv_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)
        v = get_data(batch_size, num_kv_heads, seq_len, head_dim, dtype=dtype, device="cuda", requires_grad=True)

        sm_scale = 1.0 / math.sqrt(head_dim)

        # Forward pass
        out = tile_fmha_with_backward(q, k, v, scaling=sm_scale, is_causal=is_causal)

        # Random gradient for backward
        do = torch.randn_like(out)

        # Backward pass
        out.backward(do)
        dq_cutile = q.grad.clone()
        dk_cutile = k.grad.clone()
        dv_cutile = v.grad.clone()

        # Reset gradients
        q.grad = None
        k.grad = None
        v.grad = None

        # Reference backward - need to expand K, V for PyTorch
        query_group_size = num_heads // num_kv_heads
        k_expanded = k.detach().repeat_interleave(query_group_size, dim=1)
        v_expanded = v.detach().repeat_interleave(query_group_size, dim=1)

        dq_ref, dk_ref_expanded, dv_ref_expanded = reference_backward(
            q.detach(), k_expanded, v_expanded, do, scaling=sm_scale, is_causal=is_causal
        )

        # Sum gradients for shared KV heads
        dk_ref = dk_ref_expanded.view(batch_size, num_kv_heads, query_group_size, seq_len, head_dim).sum(dim=2)
        dv_ref = dv_ref_expanded.view(batch_size, num_kv_heads, query_group_size, seq_len, head_dim).sum(dim=2)

        atol = 5e-2
        rtol = 1e-2

        torch.testing.assert_close(dq_cutile, dq_ref, atol=atol, rtol=rtol)
        torch.testing.assert_close(dk_cutile, dk_ref, atol=atol, rtol=rtol)
        torch.testing.assert_close(dv_cutile, dv_ref, atol=atol, rtol=rtol)


class Test_FMHA_Backward_Numerical(common.PyTestCase):
    """Numerical gradient tests for Flash Attention backward."""

    _backends = ["cutile"]

    @pytest.mark.parametrize("backend", _backends)
    def test_op_numerical_gradient_check(self, backend, arch):
        """Test using torch.autograd.gradcheck for numerical validation."""
        try:
            set_backend(backend)
        except Exception as e:
            pytest.skip(f"Backend is not supported: {e}")

        # Use small size and float64 for accurate gradcheck
        batch_size, num_heads, seq_len, head_dim = 1, 2, 16, 32
        dtype = torch.float64
        device = "cuda"

        q = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device, requires_grad=True)
        k = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device, requires_grad=True)
        v = get_data(batch_size, num_heads, seq_len, head_dim, dtype=dtype, device=device, requires_grad=True)

        sm_scale = 1.0 / math.sqrt(head_dim)

        def func(q, k, v):
            return FlashAttentionFunction.apply(q, k, v, sm_scale, True)

        # Note: gradcheck may be slow due to the nature of attention computation
        # Using eps=1e-6 and atol=1e-4 for reasonable tolerance
        try:
            result = torch.autograd.gradcheck(func, (q, k, v), eps=1e-4, atol=1e-3, rtol=1e-3)
            assert result, "Numerical gradient check failed"
        except Exception as e:
            # Gradient check may fail due to numerical precision in attention
            pytest.skip(f"Gradient check skipped due to numerical precision: {e}")
