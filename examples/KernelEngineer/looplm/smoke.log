python run_experiments.py

============================================================
üöÄ STARTING ADVANCED EXPERIMENT: P4_X0_Baseline
   Config: --n_embd=256 --n_head=4 --num_loops=16 --inject_x0=True --max_iters=5000
   Output: experiments/P4_X0_Baseline
============================================================
[P4_X0_Baseline] Step 1: Training for 5 iterations...
Running: python /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/train_loop.py --n_embd=256 --n_head=4 --num_loops=16 --inject_x0=True --max_iters=5000 --out_dir=experiments/P4_X0_Baseline --max_iters=5
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: num_loops = 16
Overriding: inject_x0 = True
Overriding: max_iters = 5000
Overriding: out_dir = experiments/P4_X0_Baseline
Overriding: max_iters = 5
Initializing from existing LoopLM checkpoint: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/out_addition/ckpt.pt
Vocab mismatch: Ckpt(12) vs Dataset(13).
Resetting lm_head and wte weights for new task.
Dimension mismatch or error loading state_dict: Error(s) in loading state_dict for LoopGPT:
        size mismatch for transformer.wpe.weight: copying a param with shape torch.Size([256, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
        size mismatch for transformer.h.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for transformer.h.attn.c_attn.weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([768, 256]).
        size mismatch for transformer.h.attn.c_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
        size mismatch for transformer.h.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for transformer.h.mlp.c_fc.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
        size mismatch for transformer.h.mlp.c_proj.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([256, 1024]).
        size mismatch for transformer.ln_f.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for step_embedding.weight: copying a param with shape torch.Size([12, 384]) from checkpoint, the shape in current model is torch.Size([16, 256]).
Starting from scratch instead.
Starting LoopLM Training on addition...
Config: 16 loops over 1 layer block
step 0: train loss 2.7254, val loss 2.7253, lr 0.0000e+00
iter 0: loss 2.7339, time 7828.58ms
LoopLM Training Complete. Checkpoint: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/experiments/P4_X0_Baseline/ckpt.pt

[P4_X0_Baseline] Step 2: Evaluating OOD performance (Generalization)...
Evaluating OOD for /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/experiments/P4_X0_Baseline/ckpt.pt (n=2, max_loops=None)...
OOD Accuracy: 0.00% (0/2)
‚úÖ [P4_X0_Baseline] Results: Accuracy 0.00%, Avg Steps: 16.00
[P4_X0_Baseline] Experiment completed and metrics indexed.

============================================================
üöÄ STARTING ADVANCED EXPERIMENT: P4_Pure_Dynamics
   Config: --n_embd=256 --n_head=4 --num_loops=16 --inject_x0=False --max_iters=5000
   Output: experiments/P4_Pure_Dynamics
============================================================
[P4_Pure_Dynamics] Step 1: Training for 5 iterations...
Running: python /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/train_loop.py --n_embd=256 --n_head=4 --num_loops=16 --inject_x0=False --max_iters=5000 --out_dir=experiments/P4_Pure_Dynamics --max_iters=5
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: num_loops = 16
Overriding: inject_x0 = False
Overriding: max_iters = 5000
Overriding: out_dir = experiments/P4_Pure_Dynamics
Overriding: max_iters = 5
Initializing from existing LoopLM checkpoint: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/out_addition/ckpt.pt
Vocab mismatch: Ckpt(12) vs Dataset(13).
Resetting lm_head and wte weights for new task.
Dimension mismatch or error loading state_dict: Error(s) in loading state_dict for LoopGPT:
        size mismatch for transformer.wpe.weight: copying a param with shape torch.Size([256, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
        size mismatch for transformer.h.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for transformer.h.attn.c_attn.weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([768, 256]).
        size mismatch for transformer.h.attn.c_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
        size mismatch for transformer.h.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for transformer.h.mlp.c_fc.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
        size mismatch for transformer.h.mlp.c_proj.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([256, 1024]).
        size mismatch for transformer.ln_f.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for step_embedding.weight: copying a param with shape torch.Size([12, 384]) from checkpoint, the shape in current model is torch.Size([16, 256]).
Starting from scratch instead.
Starting LoopLM Training on addition...
Config: 16 loops over 1 layer block
step 0: train loss 2.5977, val loss 2.5976, lr 0.0000e+00
iter 0: loss 2.6118, time 7048.31ms
LoopLM Training Complete. Checkpoint: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/experiments/P4_Pure_Dynamics/ckpt.pt

[P4_Pure_Dynamics] Step 2: Evaluating OOD performance (Generalization)...
Evaluating OOD for /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/experiments/P4_Pure_Dynamics/ckpt.pt (n=2, max_loops=None)...
OOD Accuracy: 0.00% (0/2)
‚úÖ [P4_Pure_Dynamics] Results: Accuracy 0.00%, Avg Steps: 16.00
[P4_Pure_Dynamics] Experiment completed and metrics indexed.

============================================================
üöÄ STARTING ADVANCED EXPERIMENT: P4_Deep_Grok
   Config: --n_embd=256 --n_head=4 --num_loops=24 --inject_x0=True --max_iters=10000 --dropout=0.3
   Output: experiments/P4_Deep_Grok
============================================================
[P4_Deep_Grok] Step 1: Training for 5 iterations...
Running: python /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/train_loop.py --n_embd=256 --n_head=4 --num_loops=24 --inject_x0=True --max_iters=10000 --dropout=0.3 --out_dir=experiments/P4_Deep_Grok --max_iters=5
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: num_loops = 24
Overriding: inject_x0 = True
Overriding: max_iters = 10000
Overriding: dropout = 0.3
Overriding: out_dir = experiments/P4_Deep_Grok
Overriding: max_iters = 5
Initializing from existing LoopLM checkpoint: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/out_addition/ckpt.pt
Vocab mismatch: Ckpt(12) vs Dataset(13).
Resetting lm_head and wte weights for new task.
Dimension mismatch or error loading state_dict: Error(s) in loading state_dict for LoopGPT:
        size mismatch for transformer.wpe.weight: copying a param with shape torch.Size([256, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
        size mismatch for transformer.h.ln_1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for transformer.h.attn.c_attn.weight: copying a param with shape torch.Size([1152, 384]) from checkpoint, the shape in current model is torch.Size([768, 256]).
        size mismatch for transformer.h.attn.c_proj.weight: copying a param with shape torch.Size([384, 384]) from checkpoint, the shape in current model is torch.Size([256, 256]).
        size mismatch for transformer.h.ln_2.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for transformer.h.mlp.c_fc.weight: copying a param with shape torch.Size([1536, 384]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
        size mismatch for transformer.h.mlp.c_proj.weight: copying a param with shape torch.Size([384, 1536]) from checkpoint, the shape in current model is torch.Size([256, 1024]).
        size mismatch for transformer.ln_f.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).
        size mismatch for step_embedding.weight: copying a param with shape torch.Size([12, 384]) from checkpoint, the shape in current model is torch.Size([24, 256]).
Starting from scratch instead.
Starting LoopLM Training on addition...
Config: 24 loops over 1 layer block
step 0: train loss 2.7251, val loss 2.7243, lr 0.0000e+00
iter 0: loss 2.7232, time 10493.81ms
LoopLM Training Complete. Checkpoint: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/experiments/P4_Deep_Grok/ckpt.pt

[P4_Deep_Grok] Step 2: Evaluating OOD performance (Generalization)...
Evaluating OOD for /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/experiments/P4_Deep_Grok/ckpt.pt (n=2, max_loops=None)...
OOD Accuracy: 0.00% (0/2)
‚úÖ [P4_Deep_Grok] Results: Accuracy 0.00%, Avg Steps: 24.00
[P4_Deep_Grok] Experiment completed and metrics indexed.

############################################################
üèÅ ALL EXPERIMENTS COMPLETE
############################################################

Summary Table:
Experiment           | Accuracy   | Avg Steps
----------------------------------------------
P4_X0_Baseline       |     0.00% |      16.00
P4_Pure_Dynamics     |     0.00% |      16.00
P4_Deep_Grok         |     0.00% |      24.00
