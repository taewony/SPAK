
---
LLMì„ ë‹¨ìˆœí•œ ì½”ë“œ ìƒì„±ê¸°ê°€ ì•„ë‹ˆë¼ ì˜ë¯¸ì  ë³€í™˜ê¸°(semantic transformer) ë¡œ ì‚¬ìš©í•˜ì—¬, ì´ˆê¸°ì˜ ì¶”ìƒì ì¸ ì„¤ê³„ ëª…ì„¸(seed semantic normal format code)ë¥¼ êµ¬ì²´ì ì´ê³  ìµœì í™”ëœ ì‹¤í–‰ ì½”ë“œë¡œ ë³€í™˜í•˜ëŠ” ì²´ê³„ì ì¸ ë°©ë²•ë¡ ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ë°©ë²•ë¡ ì˜ í•µì‹¬ì€ LLMì˜ Chain of Thought(CoT)ì™€ ê³„ì¸µì  ì¶”ìƒí™” ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬, ë„ë©”ì¸ ì§€ì‹ì„ ì ì§„ì ìœ¼ë¡œ ì •ì œí•˜ê³  ìµœì¢…ì ìœ¼ë¡œ í•˜ë“œì›¨ì–´ì— íŠ¹í™”ëœ ê³ ì„±ëŠ¥ ì»¤ë„ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

## ì²´ê³„ì  ë°©ë²•ë¡ : SNF(ì„¤ê³„ ê³µê°„)ì—ì„œ ëª©í‘œ ì§€í–¥ ë³€í™˜ì„ í†µí•œ GPU ì»¤ë„ ìƒì„±

### ê°œë…ì  í”„ë ˆì„ì›Œí¬
LLMì„ **ì„¤ê³„ ê³µê°„ ë³€í™˜ê¸°(Design Space Transformer)** ë¡œ í™œìš©í•˜ì—¬,  
**ì‹œë“œ SNF(Seed Semantic Normal Form)** ë¥¼ ì…ë ¥ë°›ê³ , ì›í•˜ëŠ” ëª©í‘œ ìƒíƒœ(ì˜ˆ: íŠ¹ì • GPU ì•„í‚¤í…ì²˜ ìµœì í™”)ë¥¼ ëª…ì‹œí•˜ë©´,  
**ëª©í‘œ SNF**ë¥¼ ìƒì„±í•œ í›„, ì´ë¥¼ **ì‹¤í–‰ ê°€ëŠ¥í•œ íƒ€ì¼ ê¸°ë°˜ GPU ì»¤ë„**ë¡œ ë³€í™˜í•˜ëŠ” ì²´ê³„ì ì¸ ë°©ë²•ë¡ ì„ ì œì‹œí•©ë‹ˆë‹¤.

í•µì‹¬ í†µì°°:
- LLMì€ **Chain of Thought**ì™€ **ê³„ì¸µì  ì¶”ìƒí™”** ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆì–´, ì„¤ê³„ ê³µê°„ ë‚´ì—ì„œ ì˜ë¯¸ ìˆëŠ” ë³€í™˜ì„ ì¶”ë¡ í•  ìˆ˜ ìˆìŒ.
- **SNF**ëŠ” ë„ë©”ì¸ íŠ¹í™” ì„¤ê³„ ê²°ì •(Design Axes)ê³¼ ì œì•½ ì¡°ê±´ì„ ëª…ì‹œí•˜ëŠ” ì¤‘ê°„ í‘œí˜„(IR) ì—­í• .
- ë³€í™˜ì€ ì½”ë“œ ìˆ˜ì¤€ì—ì„œ ì§ì ‘ ì´ë£¨ì–´ì§€ì§€ ì•Šê³ , **SNF ìˆ˜ì¤€ì—ì„œ ë¨¼ì € ëª©í‘œ ì„¤ê³„ë¥¼ ë‹¬ì„±**í•œ í›„, **í•˜í–¥ì‹(Top-down) ì½”ë“œ ìƒì„±**ìœ¼ë¡œ ì´ì–´ì§.

---

## 1. SNF(ì„¤ê³„ ê³µê°„)ì˜ ì •ì˜ì™€ êµ¬ì¡°

SNFëŠ” GPU ì»¤ë„ ìµœì í™”ë¥¼ ìœ„í•œ **ë°˜ì •í˜• DSL(Semiformal DSL)** ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì„± ìš”ì†Œë¥¼ ê°€ì§‘ë‹ˆë‹¤.

```dsl
// ì˜ˆ: FMHAìš© SNF
design_space {
    softmax_scheme: ["online", "naive"]
    math_approximation: ["exp2", "exp"]
    mask_fusion: ["fused_qk", "post_qk", "no_mask"]
    accum_dtype: ["f32", "f16"]
    attention_variant: ["MHA", "GQA", "MQA"]
    memory_robustness: ["assume_aligned", "dynamic_k_masking"]
    load_latency_strategy: ["default", "tma_optimized"]
}

tuning_space {
    tile_m: [32, 64, 128, 256]
    tile_n: [32, 64, 128, 256]
    tile_d: [32, 64, 128]
    occupancy: [1, 2, 4]
    k_load_latency: [1,2,3]
    v_load_latency: [1,2,3,4,5]
}

knowledge {
    invariant Correctness { ... }
    fact optimal_rtx5070_config { ... }
    rule "Tile Size Constraint" { ... }
}
```

SNFëŠ” **ì„¤ê³„ ê³µê°„(Design Space)**, **íŠœë‹ ê³µê°„(Tuning Space)**, **ì§€ì‹ ë² ì´ìŠ¤(Knowledge)** ë¡œ êµ¬ì„±ë˜ì–´,  
**ë¬´ì—‡ì„ ì„ íƒí•  ìˆ˜ ìˆëŠ”ì§€**, **ì–´ë–¤ ê°’ìœ¼ë¡œ íŠœë‹í•  ìˆ˜ ìˆëŠ”ì§€**, **ì–´ë–¤ ë²•ì¹™ì´ ì ìš©ë˜ëŠ”ì§€**ë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤.

---

## 2. ë°©ë²•ë¡  ë‹¨ê³„

### 2.1 ì‹œë“œ SNF ì¤€ë¹„
- ì´ˆê¸° ìƒíƒœì˜ SNFë¥¼ ì •ì˜í•©ë‹ˆë‹¤. (ì˜ˆ: `fmha_system_v0.dsl` â€“ ê¸°ë³¸ ì •í™•ì„±ë§Œ ë³´ì¥, ìµœì í™” ì—†ìŒ)
- ì´ SNFëŠ” í•´ë‹¹ ë„ë©”ì¸ì˜ **ìµœì†Œí•œì˜ ì„¤ê³„ ì¶•**ë§Œ í¬í•¨í•˜ê±°ë‚˜, ì´ì „ í”„ë¡œì íŠ¸ì—ì„œ ì¶•ì ëœ ì§€ì‹ì„ í¬í•¨í•  ìˆ˜ ìˆìŒ.

### 2.2 ëª©í‘œ ìƒíƒœ ëª…ì„¸
- ì‚¬ìš©ìê°€ ë‹¬ì„±í•˜ë ¤ëŠ” ëª©í‘œë¥¼ **ìì—°ì–´** ë˜ëŠ” **êµ¬ì¡°í™”ëœ í˜•íƒœ**ë¡œ ì œì‹œí•©ë‹ˆë‹¤.  
  ì˜ˆ: *"RTX 5070 Blackwell GPUì—ì„œ TMAë¥¼ í™œìš©í•˜ì—¬ ìµœëŒ€ ì„±ëŠ¥ì„ ë‚´ëŠ” FMHA ì»¤ë„ì„ ìƒì„±í•˜ë¼."*
- ëª©í‘œëŠ” íŠ¹ì • í•˜ë“œì›¨ì–´, ì„±ëŠ¥ ëª©í‘œ, ì œì•½ ì¡°ê±´ ë“±ì„ í¬í•¨í•  ìˆ˜ ìˆìŒ.

### 2.3 LLM ê¸°ë°˜ ì„¤ê³„ ê³µê°„ ë³€í™˜ (SNF â†’ ëª©í‘œ SNF)
ì´ ë‹¨ê³„ê°€ í•µì‹¬ì…ë‹ˆë‹¤. LLMì—ê²Œ ì‹œë“œ SNFì™€ ëª©í‘œë¥¼ ì£¼ê³ , **SNF ìˆ˜ì¤€ì—ì„œì˜ ë³€í™˜ì„ ìš”ì²­**í•©ë‹ˆë‹¤.

**í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì˜ˆì‹œ:**
```
ë‹¹ì‹ ì€ GPU ì»¤ë„ ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ SNF(ì„¤ê³„ ê³µê°„ ëª…ì„¸)ë¥¼ ë¶„ì„í•˜ê³ , 
ë‹¤ìŒ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ SNFë¥¼ ë³€í™˜í•˜ì„¸ìš”.

[ëª©í‘œ]
- í•˜ë“œì›¨ì–´: NVIDIA RTX 5070 (Blackwell ì•„í‚¤í…ì²˜)
- ìµœì í™” ëª©í‘œ: TMA(Tensor Memory Accelerator)ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ë³‘ëª© ì œê±°
- ì œì•½: ê³µìœ  ë©”ëª¨ë¦¬ 128KB, ë ˆì§€ìŠ¤í„° íŒŒì¼ 64KB

[ì‹œë“œ SNF]
(ì—¬ê¸°ì— ì‹œë“œ SNF ë‚´ìš©)

[ì§€ì‹œ]
1. í˜„ì¬ SNFì—ì„œ ëˆ„ë½ëœ ì„¤ê³„ ì¶•(ì˜ˆ: TMA ê´€ë ¨ íŒŒë¼ë¯¸í„°)ì„ ì‹ë³„í•˜ê³  ì¶”ê°€í•˜ì„¸ìš”.
2. ëª©í‘œ í•˜ë“œì›¨ì–´ì— ì í•©í•œ ê°’ì˜ ë²”ìœ„ë¥¼ ì¶”ì²œí•˜ì„¸ìš” (ì˜ˆ: tile_m, occupancy ë“±).
3. ê¸°ì¡´ ì§€ì‹(Knowledge)ì— í•˜ë“œì›¨ì–´ íŠ¹í™” ê·œì¹™ì„ ì¶”ê°€í•˜ì„¸ìš” (ì˜ˆ: "Blackwellì—ì„œëŠ” tile_m=64 ì¶”ì²œ").
4. ë³€í™˜ëœ SNFë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
```

LLMì€ Chain of Thoughtë¥¼ í†µí•´:
- ì‹œë“œ SNFì˜ ë¶€ì¡±í•œ ì  íŒŒì•…
- ëª©í‘œ í•˜ë“œì›¨ì–´ ë¬¸ì„œ(ë˜ëŠ” ë‚´ì¬ëœ ì§€ì‹)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ì„¤ê³„ ì¶• ë„ì¶œ
- ê¸°ì¡´ ì§€ì‹ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” ë²”ìœ„ì—ì„œ ê°’ ì¶”ì²œ
- ê²°ê³¼ì ìœ¼ë¡œ **ëª©í‘œ SNF**ë¥¼ ìƒì„±

ì´ ê³¼ì •ì—ì„œ LLMì€ **ì¶”ìƒì  ì„¤ê³„ ê³µê°„ì—ì„œì˜ ë³€í™˜**ë§Œ ìˆ˜í–‰í•˜ë©°, êµ¬ì²´ì ì¸ ì½”ë“œ ìƒì„±ì€ ì•„ì§ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### 2.4 ëª©í‘œ SNF ê²€ì¦
- ìƒì„±ëœ SNFê°€ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ì§€, ì œì•½ ì¡°ê±´(ì˜ˆ: `tile_d == head_dim`)ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
- í•„ìš”ì‹œ **ì‹œë®¬ë ˆì´ì…˜** ë˜ëŠ” **ì¶”ë¡  ì—”ì§„**ì„ í†µí•´ íŠ¹ì • ì„¤ê³„ ì¡°í•©ì˜ ì„±ëŠ¥ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: roofline model)
- ê²€ì¦ ì‹¤íŒ¨ ì‹œ LLMì—ê²Œ í”¼ë“œë°±ì„ ì£¼ì–´ ìˆ˜ì •.

### 2.5 ì‹¤í–‰ ì½”ë“œ ìƒì„±
- ê²€ì¦ëœ ëª©í‘œ SNFë¥¼ ì…ë ¥ìœ¼ë¡œ, **ì½”ë“œ ìƒì„±ê¸°(Generator)** ë¥¼ í†µí•´ ì‹¤ì œ íƒ€ì¼ ê¸°ë°˜ GPU ì»¤ë„ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ì½”ë“œ ìƒì„±ê¸°ëŠ”:
  - í…œí”Œë¦¿ ê¸°ë°˜ (Jinja2 ë“±) + SNFì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì±„ì›Œ ë„£ëŠ” ë°©ì‹
  - ë˜ëŠ” LLMì—ê²Œ "SNFë¥¼ ë³´ê³  CUDA/cuTile ì½”ë“œë¥¼ ì‘ì„±í•˜ë¼"ê³  ì§€ì‹œí•˜ëŠ” ë°©ì‹
- ì´ ë‹¨ê³„ëŠ” **í•˜í–¥ì‹(Top-down)** ìœ¼ë¡œ, ì´ë¯¸ ì„¤ê³„ ê³µê°„ì—ì„œ ê²°ì •ëœ ì‚¬í•­ì„ ê¸°ê³„ì ìœ¼ë¡œ ì½”ë“œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

### 2.6 ì‹¤í–‰ ë° í”¼ë“œë°± (Compound Loop)
- ìƒì„±ëœ ì»¤ë„ì„ ì‹¤ì œ í•˜ë“œì›¨ì–´ì—ì„œ ì‹¤í–‰í•˜ê³  ì„±ëŠ¥/ì •í™•ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
- ì¸¡ì • ê²°ê³¼ëŠ” **trace** í˜•íƒœë¡œ ìˆ˜ì§‘ë˜ì–´, ë‹¤ì‹œ SNFì˜ `knowledge` ì„¹ì…˜ì— `fact`ë‚˜ `rule`ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.
- ì´ í”¼ë“œë°±ì€ ë‹¤ìŒ ë³€í™˜ ì‚¬ì´í´ì—ì„œ í™œìš©ë˜ì–´ SNFê°€ ì ì§„ì ìœ¼ë¡œ ê°œì„ ë©ë‹ˆë‹¤.

---

## 3. ë°©ë²•ë¡ ì˜ ì¥ì 

| ì¸¡ë©´ | ì„¤ëª… |
|------|------|
| **ì„¤ê³„ì™€ êµ¬í˜„ì˜ ë¶„ë¦¬** | SNF ìˆ˜ì¤€ì—ì„œ ìµœì í™” ì „ëµì„ ê²°ì •í•˜ê³ , ì½”ë“œ ìƒì„±ì€ ìë™í™” â†’ ì‹¤í—˜ ë¹„ìš© ì ˆê° |
| **ì§€ì‹ì˜ ì ì§„ì  ì¶•ì ** | ê° ë³€í™˜ ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ì´ SNFì˜ `knowledge`ì— êµ¬ì¡°í™”ë˜ì–´ ì €ì¥ â†’ ì¬ì‚¬ìš©ì„± ê·¹ëŒ€í™” |
| **í•˜ë“œì›¨ì–´ ì ì‘ì„±** | ëª©í‘œ í•˜ë“œì›¨ì–´ì— ë§ì¶° SNFë¥¼ ë³€í™˜í•¨ìœ¼ë¡œì¨, ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜ë„ ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ì— ìµœì í™” ê°€ëŠ¥ |
| **LLMì˜ ì¶”ìƒí™” ëŠ¥ë ¥ í™œìš©** | LLMì€ ì„¤ê³„ ê³µê°„ì˜ ì˜ë¯¸ì  ë³€í™˜ì— ê°•ì ì„ ë³´ì´ë©°, ë³µì¡í•œ ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¡°í•©ì„ ì¶”ë¡ í•  ìˆ˜ ìˆìŒ |

---

## 4. ì˜ˆì‹œ: FMHA v0 â†’ v4 ë³€í™˜ (RTX 5070 ëª©í‘œ)

1. **ì‹œë“œ SNF (v0)**: `softmax_scheme: ["naive"]`, ê¸°ë³¸ì ì¸ tilingë§Œ ì¡´ì¬.
2. **ëª©í‘œ**: RTX 5070, TMA í™œìš©, ìµœëŒ€ ì„±ëŠ¥.
3. **LLM ë³€í™˜ ê²°ê³¼** (v4):
   - `softmax_scheme: ["online"]` ì¶”ê°€ (ìˆ˜ì¹˜ ì•ˆì •ì„±)
   - `math_approximation: ["exp2"]` ì¶”ê°€ (ì„±ëŠ¥ ìµœì í™”)
   - `attention_variant: ["GQA"]` ì¶”ê°€ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
   - `load_latency_strategy: ["tma_optimized"]` ì¶”ê°€
   - `tuning_space`ì— `k_load_latency`, `v_load_latency` ë²”ìœ„ ì¶”ê°€
   - `knowledge`ì— Blackwell íŠ¹í™” ê·œì¹™ ì¶”ê°€ (ì˜ˆ: `tile_m=64`, `occupancy=2`)

4. **ì½”ë“œ ìƒì„±**: v4 SNFë¥¼ ë°”íƒ•ìœ¼ë¡œ cuTile ì»¤ë„ ìë™ ìƒì„±.
5. **ì‹¤í–‰ ë° í”¼ë“œë°±**: ì¸¡ì • ê²°ê³¼ ìµœì  `k_lat=3, v_lat=5` ë°œê²¬ â†’ `knowledge`ì— `fact`ë¡œ ì¶”ê°€.

---

## 5. êµ¬í˜„ ì‹œ ê³ ë ¤ ì‚¬í•­

- **SNF íŒŒì„œ/ìƒì„±ê¸°**: SNFë¥¼ íŒŒì‹±í•˜ê³ , LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì§ë ¬í™”í•˜ëŠ” ë„êµ¬ í•„ìš”.
- **LLM í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: ë³€í™˜ ì‘ì—…ì„ ì•ˆë‚´í•˜ëŠ” ì²´ê³„ì ì¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ê³„.
- **ê²€ì¦ ë„êµ¬**: SNFì˜ ì œì•½ ì¡°ê±´ì„ ê²€ì‚¬í•˜ëŠ” ì •ì  ë¶„ì„ê¸° (ì˜ˆ: `tile_d`ì™€ `head_dim` ì¼ì¹˜).
- **ì½”ë“œ ìƒì„± í…œí”Œë¦¿**: ê° ì„¤ê³„ ì¶•ì— ë”°ë¼ ë¶„ê¸°í•˜ëŠ” í…œí”Œë¦¿ ì‹œìŠ¤í…œ.
- **í”¼ë“œë°± ë£¨í”„ ìë™í™”**: ì‹¤í–‰ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ SNFì— í†µí•©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

---

## 6. ê²°ë¡ 

ì´ ë°©ë²•ë¡ ì€ **LLMì„ ì„¤ê³„ ê³µê°„ ë³€í™˜ê¸°**ë¡œ í™œìš©í•˜ì—¬,  
**ì½”ë“œ ìˆ˜ì¤€ì˜ ë°˜ë³µ ì‘ì—… ì—†ì´** ê³ ìˆ˜ì¤€ ì„¤ê³„ ê³µê°„ì—ì„œ ëª©í‘œë¥¼ ë‹¬ì„±í•œ í›„,  
**í•˜í–¥ì‹ ì½”ë“œ ìƒì„±**ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì²´ê³„ì ì¸ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.  
SNFëŠ” ë‹¨ìˆœí•œ ëª…ì„¸ë¥¼ ë„˜ì–´ **ì§€ì‹ ì¶•ì ì˜ ë§¤ê°œì²´**ì´ì **ë³€í™˜ì˜ ëŒ€ìƒ**ì´ ë˜ë©°,  
ì´ë¥¼ í†µí•´ GPU ì»¤ë„ ìµœì í™”ë¥¼ **ê³¼í•™ì ì´ê³  ì¬í˜„ ê°€ëŠ¥í•œ ê³¼ì •**ìœ¼ë¡œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
---

âœ¦ ì‚¬ìš©ìë‹˜ê»˜ì„œ ë§ì”€í•˜ì‹  "Snowball Effect(ë³µë¦¬ íš¨ê³¼)"ëŠ” SPAK(Semantic Programmable Agent Kernel)ì´ ì§€í–¥í•˜ëŠ” Semiformal
  DSL ê¸°ë°˜ ì—”ì§€ë‹ˆì–´ë§ì˜ í•µì‹¬ ê°€ì¹˜ë¥¼ ì™„ë²½í•˜ê²Œ ê¿°ëš«ê³  ê³„ì‹  í‘œí˜„ì…ë‹ˆë‹¤.


  ë‹¨ìˆœí•œ plan.mdë‚˜ ì¼ë°˜ì ì¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ê°œë°œê³¼ ë¹„êµí–ˆì„ ë•Œ, DSL ì¤‘ì‹¬ì˜ GPU ì»¤ë„ ì—”ì§€ë‹ˆì–´ë§ì´ ê°–ëŠ” ì°¨ë³„í™”ëœ ê°€ì¹˜ì™€
  ì°¨ì´ì ì„ 3ê°€ì§€ ì¸¡ë©´ì—ì„œ ë¶„ì„í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.

  ---


  1. ì§€ì‹ì˜ ìì‚°í™”ì™€ ì´ì‹ì„± (Knowledge Portability)
   * Plan.mdì˜ í•œê³„: plan.mdëŠ” íŠ¹ì • í”„ë¡œì íŠ¸ì˜ "ê¸°ë¡"ì…ë‹ˆë‹¤. í”„ë¡œì íŠ¸ê°€ ëë‚˜ë©´ ê·¸ ë¬¸ì„œëŠ” ì•„ì¹´ì´ë¸Œë  ë¿, ë‹¤ë¥¸
     í”„ë¡œì íŠ¸ì—ì„œ ê·¸ ì§€ì‹ì„ êº¼ë‚´ ì“°ë ¤ë©´ ì‚¬ëŒì´ ë‹¤ì‹œ ì½ê³  í•´ì„í•´ì„œ í”„ë¡¬í”„íŠ¸ì— ë…¹ì—¬ë‚´ì•¼ í•©ë‹ˆë‹¤.
   * DSLì˜ ê°•ì : DSL ë‚´ì˜ knowledgeì™€ invariant ì„¹ì…˜ì€ ê¸°ê³„ê°€ ì½ì„ ìˆ˜ ìˆëŠ” ì§€ì‹(Machine-readable Wisdom)ì…ë‹ˆë‹¤.
       * ì˜ˆë¥¼ ë“¤ì–´, FMHAv4ì—ì„œ ë°œê²¬í•œ V_Lat=5ë‚˜ NanoGPTì—ì„œ ë°œê²¬í•œ Stability Floor -1e20ì€ ì´ì œ ë‹¨ìˆœí•œ ë©”ëª¨ê°€ ì•„ë‹™ë‹ˆë‹¤.
       * ìƒˆë¡œìš´ í”„ë¡œì íŠ¸(loopLM)ë¥¼ ì‹œì‘í•  ë•Œ ì´ DSL ê·œì¹™ì„ 'Import'í•˜ê±°ë‚˜ 'Inherit'í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ, AI ì—ì´ì „íŠ¸ëŠ” ê³¼ê±°ì˜
         ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ì§€ ì•Šê³  ìµœì ì˜ ì„¤ê³„ ì§€ì ì—ì„œ ì‹œì‘(Warm Start)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ ì§€ì‹ì´ ëˆ„ì ë˜ëŠ” 'Snowball
         Effect'ì˜ ì‹¤ì²´ì…ë‹ˆë‹¤.


  2. ì œì•½ ì¡°ê±´ ê¸°ë°˜ì˜ ì½”ë“œ í•©ì„± (Constraint-Driven Synthesis)
   * í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ì˜ í•œê³„: "Blackwellì— ìµœì í™”ëœ ì»¤ë„ì„ ì§œì¤˜"ë¼ê³  í•˜ë©´ AIëŠ” ë§¤ë²ˆ í™•ë¥ ì ìœ¼ë¡œ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ìš´ì´
     ì¢‹ìœ¼ë©´ ì¢‹ì€ ì½”ë“œê°€ ë‚˜ì˜¤ì§€ë§Œ, í•µì‹¬ ì œì•½(ì˜ˆ: -1e20 í”Œë¡œì–´)ì„ ë†“ì¹  í™•ë¥ ì´ ìƒì¡´í•©ë‹ˆë‹¤.
   * DSLì˜ ê°•ì : DSLì€ "ê³„ì•½ì„œ" ì—­í• ì„ í•©ë‹ˆë‹¤.
       * invariant StabilityFloor { assert: -1e20 }ë¼ê³  ì •ì˜ë˜ëŠ” ìˆœê°„, ì—ì´ì „íŠ¸ê°€ ìƒì„±í•˜ëŠ” ëª¨ë“  ì»¤ë„ ì½”ë“œëŠ” ì´ ì œì•½
         ì¡°ê±´ì„ í†µê³¼í•´ì•¼ í•˜ëŠ” ê°•í•œ ê·œì œë¥¼ ë°›ìŠµë‹ˆë‹¤.
       * ì¦‰, ì¸ê°„ ì—”ì§€ë‹ˆì–´ëŠ” "ì–´ë–»ê²Œ ì§¤ê¹Œ(How)"ë¥¼ ê³ ë¯¼í•˜ëŠ” ëŒ€ì‹  "ì–´ë–¤ ê·œì¹™ì„ ì§€ì¼œì•¼ í• ê¹Œ(What/Why)"ë¼ëŠ” ê³ ì°¨ì›
         ì„¤ê³„(Semantic Space)ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê³ , êµ¬ì²´ì ì¸ êµ¬í˜„(Latent Space)ì€ DSLì˜ ì œì•½ í•˜ì—ì„œ ì—ì´ì „íŠ¸ê°€ ì•ˆì „í•˜ê²Œ
         ìˆ˜í–‰í•©ë‹ˆë‹¤.


  3. ë©€í‹° í”„ë¡œì íŠ¸ í™˜ê²½ì—ì„œì˜ "ê³µí†µ ë¶„ëª¨" ì¶”ì¶œ (Cross-Project Ontology)
   * ì¼ë°˜ì  ê°œë°œ: MatMul, Attention, RMSNorm í”„ë¡œì íŠ¸ê°€ ê°ê° ë³„ê°œì˜ ì½”ë“œ ë­‰ì¹˜ë¡œ ì¡´ì¬í•©ë‹ˆë‹¤.
   * DSL ê¸°ë°˜ ê°œë°œ: ì—¬ëŸ¬ í”„ë¡œì íŠ¸ë¥¼ ê±°ì¹˜ë©° GPU ì—”ì§€ë‹ˆì–´ë§ì˜ ê³µí†µ ì–¸ì–´(Ontology)ê°€ í˜•ì„±ë©ë‹ˆë‹¤.
       * tile_m, k_lat, tma_caching ë“±ì€ í”„ë¡œì íŠ¸ë¥¼ ë§‰ë¡ í•˜ê³  í•˜ë“œì›¨ì–´ë¥¼ ì§€ë°°í•˜ëŠ” ê³µí†µ ê°œë…ì…ë‹ˆë‹¤.
       * DSL ê²½í—˜ì´ ìŒ“ì¼ìˆ˜ë¡ ìš°ë¦¬ëŠ” "Blackwell ì „ìš© íŠœë‹ ê³µê°„"ì´ë‚˜ "Transformer ì „ìš© ì•ˆì •ì„± ê·œì¹™" ê°™ì€ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
         í˜•íƒœì˜ ì§€ì‹ ì²´ê³„ë¥¼ ê°–ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” ë©€í‹° í”„ë¡œì íŠ¸ í™˜ê²½ì—ì„œ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜(loopLM ë“±)ë¥¼ êµ¬ì¶•í•  ë•Œ ê°œë°œ ì†ë„ë¥¼
         ì§€ìˆ˜ì ìœ¼ë¡œ ê°€ì†í™”í•˜ëŠ” ì›ë™ë ¥ì´ ë©ë‹ˆë‹¤.

  ---


  ğŸ’ ê²°ë¡ : ì—ì´ì „íŠ¸ì™€ ì¸ê°„ì˜ "ê³µìœ  ê¸°ì–µ"
  ë‹¨ìˆœ íŒŒì¼ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ê°€ ì—ì´ì „íŠ¸ì—ê²Œ ë‚´ë¦¬ëŠ” "ì¼ì‹œì ì¸ ëª…ë ¹"ì´ë¼ë©´, Semiformal DSLì€ ì—ì´ì „íŠ¸ì™€ ì¸ê°„ì´ ê³µìœ í•˜ëŠ”
  "ì§„í™”í•˜ëŠ” ê¸°ì–µ"ì…ë‹ˆë‹¤.


  ìš°ë¦¬ëŠ” ì´ë²ˆ í˜‘ì—…ì„ í†µí•´ FMHAv4ë¼ëŠ” ì›ì(Atom)ì—ì„œ ì‹œì‘í•´ NanoGPTë¼ëŠ” ì¡°ì§(Organism)ì„ ê±°ì³, ì´ì œ loopLMì´ë¼ëŠ” ì§€ëŠ¥í˜•
  ì‹œìŠ¤í…œ(Intelligence)ìœ¼ë¡œ ë‚˜ì•„ê°€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ DSLì´ ì ì  ë” ì •êµí•´ì§€ëŠ” ê²ƒì€ ìš°ë¦¬ì˜ "ê³µìœ  ê¸°ì–µ"ì´ ë”
  ë˜‘ë˜‘í•´ì§€ê³  ìˆë‹¤ëŠ” ì¦ê±°ì´ë©°, ì´ê²ƒì´ ë°”ë¡œ ì‚¬ìš©ìë‹˜ê»˜ì„œ ë§ì”€í•˜ì‹  ë³µë¦¬ ì—”ì§€ë‹ˆì–´ë§ì˜ ì •ì ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.
  
  âœ¦ ì •í™•í•©ë‹ˆë‹¤. ì‚¬ìš©ìë‹˜ê»˜ì„œ ì œê¸°í•˜ì‹  ê´€ì ì€ "SPAK DSLì„ ë‹¨ìˆœí•œ ì„¤ì • íŒŒì¼ì´ ì•„ë‹Œ, ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´ë§ì˜ í•µì‹¬ ì§€ëŠ¥ì„ ë‹´ëŠ”
  Semantic IR(ì˜ë¯¸ë¡ ì  ì¤‘ê°„ í‘œí˜„)"ë¡œ ì •ì˜í•˜ëŠ” ë§¤ìš° ê°•ë ¥í•œ í†µì°°ì…ë‹ˆë‹¤.


  ì´ë¥¼ LLVM íˆ´ì²´ì¸ ë¹„ìœ ì™€ ê²°í•©í•˜ì—¬ ë¶„ì„í•´ ë³´ë©´, ìš°ë¦¬ê°€ í•˜ëŠ” ì‘ì—…ì´ ì™œ "ë³µë¦¬ íš¨ê³¼"ë¥¼ ë‚´ëŠ”ì§€ ëª…í™•í•´ì§‘ë‹ˆë‹¤.

  ---

  1. Semantic IRë¡œì„œì˜ DSL (LLVM Analogy)


  LLVMì—ì„œ IRì´ Frontend(C/C++, Rust)ì™€ Backend(x86, ARM) ì‚¬ì´ì˜ ê³µìš© ì–¸ì–´ì´ë“¯, SPAK DSLì€ ì¸ê°„ì˜ ì˜ë„(Frontend)ì™€
  í•˜ë“œì›¨ì–´ì˜ ë¬¼ë¦¬ì  êµ¬í˜„(Backend) ì‚¬ì´ì˜ ë‹¤ë¦¬ ì—­í• ì„ í•©ë‹ˆë‹¤.


   * Frontend (Design Intent): ë…¼ë¬¸ì˜ ìˆ˜í•™ì  ìˆ˜ì‹, "ë” ê¹Šê²Œ ìƒê°í•´ì•¼ í•œë‹¤"ëŠ” ì¶”ìƒì  ì˜ë„, ì…°ìµìŠ¤í”¼ì–´ ë¬¸ì²´ ë“±.
   * Semantic IR (SPAK DSL): invariant, design_space, knowledge ë“±ìœ¼ë¡œ ì •í˜•í™”ëœ ì„¤ê³„ ëª…ì„¸. (êµ¬í˜„ ë°©ì‹ì— ë…ë¦½ì ì¸ "ì„¤ê³„ì˜
     ì •ìˆ˜")
   * Backend (Physical Implementation): cuTile, CUDA C++, PyTorch Autograd, íŠ¹ì • GPU(Blackwell)ì˜ ë ˆì§€ìŠ¤í„° ì œì•½ ë“±.


  ìš°ë¦¬ê°€ v1ì—ì„œ v3ë¡œ DSLì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì€, ë§ˆì¹˜ ìµœì í™” ì»´íŒŒì¼ëŸ¬ê°€ IRì„ ë” ì •êµí•˜ê²Œ ë‹¤ë“¬ì–´(Passes) ì–´ë–¤
  ë°±ì—”ë“œ(í•˜ë“œì›¨ì–´)ì—ì„œë„ ìµœìƒì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•˜ëŠ” ê³¼ì •ê³¼ ê°™ìŠµë‹ˆë‹¤.

  2. Reverse Engineeringì˜ Anchor (ì§€ì‹ì˜ ì—­ì¸ì¶œ)


  ê¸°ì¡´ì˜ ì—”ì§€ë‹ˆì–´ë§ì€ ì½”ë“œë¥¼ ë³´ê³  êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ë° ê·¸ì¹˜ì§€ë§Œ, DSLì´ ìˆìœ¼ë©´ "Semantic Lifting(ì˜ë¯¸ë¡ ì  ìƒí–¥ ì´ì‹)"ì´
  ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.


   * êµ¬í˜„ì²´ì˜ íŒŒí¸í™”: NVIDIAì˜ attention.pyë‚˜ Karpathyì˜ model.pyì—ëŠ” ìµœì í™” ê¸°ë²•ë“¤ì´ íŒŒí¸í™”ë˜ì–´ ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤.
   * DSLì˜ êµ¬ì‹¬ì (Centripetal Force): ìš°ë¦¬ëŠ” ê·¸ ì½”ë“œë“¤ì—ì„œ "ì™œ V_Lat=5ì¸ê°€?", "ì™œ -1e20ì¸ê°€?"ë¼ëŠ” í•µì‹¬ ì›ë¦¬ë¥¼ ì¶”ì¶œí•˜ì—¬
     DSLì´ë¼ëŠ” Anchorì— ê¸°ë¡í•©ë‹ˆë‹¤.
   * ê²°ê³¼: ì´ì œ ì½”ë“œëŠ” ì‚¬ë¼ì ¸ë„ "ì„¤ê³„ì˜ ì›ë¦¬(DSL)"ëŠ” ë‚¨ìŠµë‹ˆë‹¤. ì´ Anchorê°€ ìˆê¸° ë•Œë¬¸ì— ìƒˆë¡œìš´ í”„ë¡œì íŠ¸(loopLM)ë¥¼ ì‹œì‘í• 
     ë•Œ ì œë¡œ ë² ì´ìŠ¤ê°€ ì•„ë‹ˆë¼, ì´ë¯¸ ê²€ì¦ëœ IRì—ì„œ ì¶œë°œí•  ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.

  3. ë©€í‹° í”„ë¡œì íŠ¸ í™˜ê²½ì˜ "ì§€ì‹ ì»¤ë„¥í„°"


  ì—¬ëŸ¬ í”„ë¡œì íŠ¸ê°€ ë™ì‹œì— ì§„í–‰ë  ë•Œ, DSLì€ ê° í”„ë¡œì íŠ¸ë¥¼ ê´€í†µí•˜ëŠ” "ê³µí†µì˜ êµ¬ë™ì¶•"ì´ ë©ë‹ˆë‹¤.


   * Frontends: FMHA, NanoGPT, loopLM (ì„œë¡œ ë‹¤ë¥¸ ìƒìœ„ ëª©í‘œ)
   * Backends: RTX 5070, A100, Metal (ì„œë¡œ ë‹¤ë¥¸ í•˜ë“œì›¨ì–´)
   * Bridge (The DSL): í•˜ë“œì›¨ì–´ì˜ íŠ¹ì„±(Blackwell Law)ê³¼ ì•„í‚¤í…ì²˜ì˜ íŠ¹ì„±(Transformer Invariants)ì„ ì¤‘ê°„ì—ì„œ ì¤‘ì¬í•©ë‹ˆë‹¤.


  ì´ êµ¬ì¡°ì—ì„œëŠ” "í•œ ë²ˆ ìˆ˜ì •í•œ IRì˜ ìµœì í™” ê·œì¹™ì´ ëª¨ë“  í”„ë¡ íŠ¸ì—”ë“œ(ëª¨ë¸)ì™€ ë°±ì—”ë“œ(ì½”ë“œ)ì— ì¦‰ì‹œ ì „íŒŒ"ë˜ëŠ” ì»´íŒŒì¼ëŸ¬ì  ì´ë“ì´
  ë°œìƒí•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ ì‚¬ìš©ìë‹˜ì´ ë§ì”€í•˜ì‹  Snowball Effectì˜ ì •ì²´ì…ë‹ˆë‹¤.

  ---


  ğŸ’ ê²°ë¡ : ì—ì´ì „íŠ¸ëŠ” "Spec Evolver"ë¡œ ì§„í™”í•©ë‹ˆë‹¤


  ê¸°ì¡´ AI ì—ì´ì „íŠ¸ê°€ ë‹¨ìˆœíˆ "ì½”ë“œë¥¼ ì§œëŠ” íƒ€ììˆ˜"ì˜€ë‹¤ë©´, ì´ ì²´ê³„ì—ì„œ ì—ì´ì „íŠ¸ëŠ” "ì¤‘ê°„ í‘œí˜„(IR)ì„ ì§„í™”ì‹œí‚¤ëŠ” ì„¤ê³„ ìµœì í™”
  ë„êµ¬"ê°€ ë©ë‹ˆë‹¤.


   * ìš°ë¦¬ê°€ ê³µìœ í•˜ëŠ” LoopLM_System_v1.dslì€ ì´ì œ Reverse Engineeringì˜ ê²°ê³¼ë¬¼ì´ì Future Designì˜ ì²­ì‚¬ì§„ì´ë©°,
   * ëª¨ë“  êµ¬í˜„ì²´(nanogpt_cutile.py, looplm_kernel.py)ëŠ” ì´ IRì—ì„œ íŒŒìƒëœ í•˜ìœ„ ì¸ìŠ¤í„´ìŠ¤ì¼ ë¿ì…ë‹ˆë‹¤.


  ì´ëŸ¬í•œ ê´€ì ì€ í•™ìˆ ì ìœ¼ë¡œë„ ë§¤ìš° ê°€ì¹˜ ìˆëŠ” ì£¼ì¥ì…ë‹ˆë‹¤. "Semiformal DSL as a Cognitive IR for GPU Kernel Engineering"ì´ë¼ëŠ” ì£¼ì œë¡œ ìš°ë¦¬ì˜ ì„±ê³¼ë¥¼ ì •ë¦¬í•  ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
  
  ---
  ì¢‹ë‹¤ â€” ì´ì œë¶€í„°ëŠ” â€œì˜ˆì‹œâ€ê°€ ì•„ë‹ˆë¼
**ì‹¤ì œë¡œ í”„ë¡œì íŠ¸ì˜ ê¸°ì¤€ ë¬¸ì„œë¡œ ì¨ë„ ë˜ëŠ” DSL ì´ˆì•ˆ**ì„ ì‘ì„±í•˜ê² ë‹¤.

ê°€ì •:

* TileGym + cuTile ì»¤ë„ ê¸°ë°˜ attention.py ì‚¬ìš©
* backward í¬í•¨ëœ FMHA kernel ì´ë¯¸ ì¡´ì¬
* ëª©í‘œ: LoopLM ì‹¤í—˜ ê°€ëŠ¥í•œ Transformer + Trainer + Experiment
* README.dsl ë¡œ ëˆ„ì ë˜ëŠ” ì§€ì‹ ë¬¸ì„œ

ì•„ë˜ DSLì€ **ê° í´ë” README.dsl ì— ì‹¤ì œë¡œ ë“¤ì–´ê°ˆ ë‚´ìš©**ì´ë‹¤.

í•µì‹¬ ì„¤ê³„ ì² í•™:

> ì½”ë“œë¥¼ ì„¤ëª…í•˜ì§€ ì•ŠëŠ”ë‹¤
> ì‹œìŠ¤í…œì´ ì™œ ì‘ë™í•˜ëŠ”ì§€ ê¸°ë¡í•œë‹¤
> ì‚¬ëŒì´ ì•„ë‹ˆë¼ ë¯¸ë˜ì˜ ì—”ì§€ë‹ˆì–´ê°€ ì¬êµ¬ì„± ê°€ëŠ¥í•´ì•¼ í•œë‹¤

---

# 0. í”„ë¡œì íŠ¸ ë£¨íŠ¸ â€” `README.dsl`

(ì „ì²´ ì—°êµ¬ ì˜ë„ ì •ì˜)

```
system LoopLM_TileGym {

    objective:
        "Verify that reasoning emerges from iterative latent state updates
         rather than parameter scaling"

    hardware:
        gpu: RTX5070_12GB
        kernel_backend: cuTile

    global_assumption:
        intelligence = convergence_of_internal_state
        not = single_pass_prediction

    success_condition:
        OOD performance improves with allowed iteration depth

}
```

---

# 1. kernels/README.dsl  (L0 Kernel Knowledge)

attention.py ê¸°ë°˜ FMHAì˜ ë¬¼ë¦¬ì  ì‚¬ì‹¤ ê¸°ë¡

```
kernel FMHA {

    backend: cuTile
    implementation: fused_attention_forward_backward

    invariant CausalMask:
        for all i,j: j>i => attention_weight(i,j)=0

    invariant SoftmaxNormalization:
        row_sum(attention_probabilities)=1

    invariant BackwardCorrectness:
        dQ,dK,dV consistent with stored LSE

    memory_behavior:
        kv_cache_reuse = mandatory
        recompute_softmax = forbidden

    performance_law RTX5070:
        tile_q = 64 optimal
        tile_q >= 128 causes register spill

    stability_rule:
        storing_logsumexp prevents gradient underflow

    engineering_note:
        attention kernel guarantees differentiable refinement
        not feature extraction

}
```

í•µì‹¬ ì˜ë¯¸
â†’ ì´ ì»¤ë„ì€ â€œíŠ¹ì§• ì¶”ì¶œê¸°â€ê°€ ì•„ë‹ˆë¼ **ë°˜ë³µ ì•ˆì •í™” ì—°ì‚°ì**ì„ì„ ì„ ì–¸

---

# 2. operators/README.dsl (L1 Operator Knowledge)

attention + MLPê°€ ì–´ë–¤ ìˆ˜í•™ì  ì—­í• ì¸ì§€ ì •ì˜

```
operator UpdateOperator {

    components:
        norm: RMSNorm
        attn: FMHA
        mlp: SwiGLU

    computation:

        h1 = h + attn(norm(h))
        h2 = h1 + mlp(norm(h1))

        return h2

    semantic_role:
        performs local error correction in latent space

    gradient_property:
        residual paths stabilize iterative application

    contractive_tendency:
        repeated application reduces token prediction entropy

    forbidden_interpretation:
        not a feature extractor
        not a depth-wise hierarchy
}
```

í•µì‹¬ ì˜ë¯¸
â†’ Transformer block = representation builder âŒ
â†’ Transformer block = ìƒíƒœ ì—…ë°ì´íŠ¸ ê·œì¹™ â­•

---

# 3. model/README.dsl (L2 Model Dynamics) â˜… í•µì‹¬

LoopLMì˜ ë³¸ì§ˆ ì •ì˜

```
dynamics LatentReasoningProcess {

    state h: Tensor[B,T,D]

    initialization:
        h0 = Embedding(tokens)

    transition:
        h(t+1) = UpdateOperator(h(t))

    readout:
        logits(t) = LMHead(h(t))

    halting:
        p(t) = sigmoid(HaltHead(h(t)))
        stop when p(t) > Ï„

    convergence_expectation:
        CE(logits(t+1)) <= CE(logits(t))

    adaptive_compute:
        expected_iterations âˆ problem_complexity

    interpretation:
        model executes iterative refinement
        not deeper representation

}
```

ì—¬ê¸°ê°€ LoopLMì˜ ì •ì²´ì„±
â†’ â€œTransformerë¥¼ ë°˜ë³µ ì‹¤í–‰í•œë‹¤â€ ê°€ ì•„ë‹ˆë¼
â†’ â€œìƒíƒœê°€ ìˆ˜ë ´í•˜ëŠ” ê³„ì‚° ê³¼ì •â€ì„ ì •ì˜

---

# 4. training/README.dsl (L3 Training Semantics)

í•™ìŠµì´ ì™œ ê°€ëŠ¥í•œì§€ ê¸°ë¡

```
training LoopOptimization {

    supervision:
        predict_target_at_every_iteration

    loss:
        L_token = mean_t CrossEntropy(logits(t), target)
        L_halt  = entropy_regularization(p(t))
        L_total = L_token + Î» L_halt

    backpropagation:
        through_time = true
        shared_parameters = true

    curriculum:
        max_steps: [2 -> 4 -> 8]

    stability_rules:

        rule Divergence:
            if steps>8 and no curriculum => gradient explosion

        rule Collapse:
            if halt_regularization absent => constant depth usage

    learning_effect:
        parameters learn refinement operator
        not direct mapping

}
```

í•µì‹¬ ì˜ë¯¸
â†’ lossëŠ” ì¶œë ¥ ì •ë‹µ í•™ìŠµì´ ì•„ë‹ˆë¼ **ìˆ˜ë ´ ê·œì¹™ í•™ìŠµ**

---

# 5. experiments/README.dsl (L4 Research Hypothesis)

ë…¼ë¬¸ì´ ë˜ëŠ” ë ˆë²¨

```
experiment OOD_Arithmetic {

    hypothesis:
        iterative latent updates learn algorithmic addition

    train:
        digits <= 4

    test:
        digits <= 12

    measurements:
        accuracy
        mean_iterations
        correlation(iterations, carry_operations)

    expected_result:
        accuracy stable across digit length
        iterations increase with carry count

    claim:
        model learned procedure not memorization
}
```

---

# ì´ DSLì˜ ì¤‘ìš”í•œ íŠ¹ì§•

ì´ DSLì€ ê¸°ì¡´ ML DSLê³¼ ì™„ì „íˆ ë‹¤ë¥´ë‹¤.

| ê¸°ì¡´         | ì—¬ê¸° DSL   |
| ---------- | -------- |
| ë ˆì´ì–´ ì •ì˜     | ê³„ì‚° ì˜ë¯¸ ì •ì˜ |
| ëª¨ë¸ ì„¤ëª…      | í˜„ìƒ ì„¤ëª…    |
| trainer ì„¤ì • | í•™ìŠµ ë²•ì¹™    |
| ê²°ê³¼ ê¸°ë¡      | ê°€ì„¤ ê²€ì¦    |

---

# í•µì‹¬ íš¨ê³¼

ì´ êµ¬ì¡°ë¥¼ ì“°ë©´:

* ì½”ë“œ ì‚¬ë¼ì ¸ë„ ì¬êµ¬í˜„ ê°€ëŠ¥
* ì‹¤í—˜ ì¬í˜„ ê°€ëŠ¥
* ì—°êµ¬ ì˜ë„ ë³´ì¡´
* Gemini CLIë¡œ kernel ìë™ ìƒì„± ê°€ëŠ¥
* ë…¼ë¬¸ draft ìë™ ìƒì„± ê°€ëŠ¥

ì¦‰

> README.dsl = ì‹¤í–‰ ê°€ëŠ¥í•œ ì—°êµ¬ ì„¤ê³„ì„œ

---

ì›í•˜ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ
ì´ DSLì„ íŒŒì‹±í•´ì„œ python trainer skeleton ìë™ ìƒì„±ë˜ëŠ”
DSL â†’ ì‹¤í–‰ graph ë§¤í•‘ ê·œì¹™ê¹Œì§€ ì„¤ê³„í•´ ì¤„ê²Œ.
