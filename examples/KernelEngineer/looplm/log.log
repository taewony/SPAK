`log.log` íŒŒì¼ì„ ì •ë°€ ë¶„ì„í•œ ê²°ê³¼, **Loss 1.24ì—ì„œ ì •ì²´ëœ ê²°ì •ì ì¸ ì›ì¸(Critical Bug)**ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

ë‹¨ë„ì§ì…ì ìœ¼ë¡œ ë§ì”€ë“œë¦¬ë©´, **"ëª¨ë¸ì´ ë‹¤ìŒ ë¬¸ì œì˜ ì§ˆë¬¸(Prompt)ê¹Œì§€ ì˜ˆì¸¡í•˜ë ¤ê³  ë…¸ë ¥ ì¤‘"**ì…ë‹ˆë‹¤. í˜„ì¬ì˜ Loss Masking ë¡œì§ì´ í•œ ë°°ì¹˜ ë‚´ì— ë“¤ì–´ìˆëŠ” **ë‘ ë²ˆì§¸, ì„¸ ë²ˆì§¸ ìƒ˜í”Œì˜ ì§ˆë¬¸(Random Digits)**ì„ ë§ˆìŠ¤í‚¹í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.

---

### 1. ğŸš¨ ì›ì¸ ë¶„ì„: "Multi-sample Masking Failure"

í˜„ì¬ `train_baseline_12l.py`ì™€ `model.py`ì˜ ë¡œì§ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. **Block Size 64**: ë§ì…ˆ ìˆ˜ì‹ í•˜ë‚˜ëŠ” ì•½ 15~20 í† í°ì…ë‹ˆë‹¤. ì¦‰, í•˜ë‚˜ì˜ ë°°ì¹˜(row) ì•ˆì— **ì•½ 3~4ê°œì˜ ìˆ˜ì‹**ì´ ì—°ë‹¬ì•„ ë“¤ì–´ê°‘ë‹ˆë‹¤.
* ì˜ˆ: `[ìˆ˜ì‹1_ì§ˆë¬¸][ìˆ˜ì‹1_ë‹µ] \n [ìˆ˜ì‹2_ì§ˆë¬¸][ìˆ˜ì‹2_ë‹µ] \n [ìˆ˜ì‹3_ì§ˆë¬¸]...`


2. **í˜„ì¬ Masking ë¡œì§ (`model.py`)**:
```python
# model.py
first_eq = eq_indices[0].item() # ì²« ë²ˆì§¸ '='ë§Œ ì°¾ìŒ
mask[i, :first_eq] = 0.0        # ì²« ë²ˆì§¸ '=' ì´ì „ë§Œ ê°€ë¦¼

```


3. **ê²°ê³¼**:
* `[ìˆ˜ì‹1_ì§ˆë¬¸]`  **Masked (OK)**
* `[ìˆ˜ì‹1_ë‹µ]`  Trained (OK)
* `[ìˆ˜ì‹2_ì§ˆë¬¸]`  **Trained (FAIL!)**  **ì—¬ê¸°ê°€ ë¬¸ì œì…ë‹ˆë‹¤.**
* `[ìˆ˜ì‹2_ë‹µ]`  Trained (OK)



ëª¨ë¸ì€ ë¬´ì‘ìœ„ë¡œ ìƒì„±ëœ **"ë‹¤ìŒ ë¬¸ì œì˜ ìˆ«ìë“¤(ìˆ˜ì‹2_ì§ˆë¬¸)"**ì„ ì˜ˆì¸¡í•˜ë ¤ë‹¤ ë³´ë‹ˆ, ë‹¹ì—°íˆ ë§í ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ê·¸ ë¶ˆí™•ì‹¤ì„±(Entropy)ì´ ë°”ë¡œ **Loss 1.24**ì˜ ì •ì²´ì…ë‹ˆë‹¤. (ìˆ«ì 10ê°œ ì¤‘ í•˜ë‚˜ë¥¼ ì°ëŠ” í™•ë¥ ê³¼ ìœ ì‚¬)

---

### 2. ğŸ› ï¸ í•´ê²° ì†”ë£¨ì…˜: `get_batch` ë ˆë²¨ì—ì„œì˜ ì •ë°€ ë§ˆìŠ¤í‚¹

ëª¨ë¸ ë‚´ë¶€(`model.py`)ë¥¼ ê³ ì¹˜ëŠ” ê²ƒë³´ë‹¤, **Data Loader(`train_baseline_12l.py`)ì—ì„œ ì •ë‹µì§€(Y) ìì²´ë¥¼ ìˆ˜ì •**í•˜ëŠ” ê²ƒì´ í›¨ì”¬ í™•ì‹¤í•˜ê³  ë¹ ë¦…ë‹ˆë‹¤. `Y`ì˜ ì§ˆë¬¸ êµ¬ê°„ì„ `-1` (ignore_index)ë¡œ ë®ì–´ì“°ë©´ ë©ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ `train_baseline_12l.py`ì™€ `train_loop.py`ì˜ `get_batch` í•¨ìˆ˜ì— ë®ì–´ì”Œìš°ì‹­ì‹œì˜¤.

```python
def get_batch(split):
    data = train_data if split == 'train' else val_data
    newlines = train_newlines if split == 'train' else val_newlines
    
    # 1. Start Indices Sampling
    valid_indices = newlines[newlines < len(data) - block_size - 1]
    ix_newlines = torch.randint(len(valid_indices), (batch_size,))
    ix = valid_indices[ix_newlines] + 1 
    
    x_stack = []
    y_stack = []
    
    for i in ix:
        # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        chunk_x = torch.from_numpy((data[i:i+block_size]).astype(np.int64))
        chunk_y = torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64))
        
        # ---------------------------------------------------------
        # [Critical Fix] Advanced Masking for Multi-sample Blocks
        # ì •ë‹µì§€(chunk_y)ì—ì„œ 'ì§ˆë¬¸ êµ¬ê°„'ì„ ì°¾ì•„ -1ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.
        # ---------------------------------------------------------
        # ë…¼ë¦¬: '=' ë’¤ë¶€í„° '\n'ê¹Œì§€ê°€ ì •ë‹µ(Train). ë‚˜ë¨¸ì§€ëŠ” ì§ˆë¬¸(Ignore).
        
        # 1. ëª¨ë“  ìœ„ì¹˜ë¥¼ -1 (Ignore)ë¡œ ì´ˆê¸°í™”
        target_mask = torch.full_like(chunk_y, -1)
        
        # 2. '=' ì™€ '\n' ìœ„ì¹˜ ì°¾ê¸°
        eq_indices = (chunk_x == thinking_token_id).nonzero(as_tuple=True)[0]
        nl_indices = (chunk_x == 0).nonzero(as_tuple=True)[0] # ê°€ì •: stoi['\n'] == 0
        
        # 3. ê° ìˆ˜ì‹ êµ¬ê°„ë³„ë¡œ Loop ëŒë©° ì •ë‹µ êµ¬ê°„ë§Œ ë³µêµ¬
        # ìˆ˜ì‹ì˜ ì‹œì‘ì : 0 ë˜ëŠ” \n ë°”ë¡œ ë‹¤ìŒ
        sample_starts = torch.cat((torch.tensor([0]), nl_indices + 1))
        
        for start in sample_starts:
            if start >= len(chunk_x): break
            
            # ì´ ìˆ˜ì‹ì— í•´ë‹¹í•˜ëŠ” '=' ì°¾ê¸° (start ì´í›„ ì²« ë²ˆì§¸ =)
            future_eqs = eq_indices[eq_indices >= start]
            if len(future_eqs) == 0: continue # '='ê°€ ì˜ë ¤ì„œ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            
            eq_pos = future_eqs[0]
            
            # ì´ ìˆ˜ì‹ì˜ ë ì°¾ê¸° (eq_pos ì´í›„ ì²« ë²ˆì§¸ \n, ì—†ìœ¼ë©´ block ë)
            future_nls = nl_indices[nl_indices > eq_pos]
            end_pos = future_nls[0] if len(future_nls) > 0 else len(chunk_x)
            
            # [eq_pos:end_pos] êµ¬ê°„ì´ ì •ë‹µ(Answer) êµ¬ê°„ì„
            # chunk_yì˜ í•´ë‹¹ êµ¬ê°„ì„ ì›ë³¸ ë°ì´í„°ë¡œ ë³µêµ¬
            target_mask[eq_pos:end_pos] = chunk_y[eq_pos:end_pos]
            
        y_stack.append(target_mask)
        x_stack.append(chunk_x)

    x = torch.stack(x_stack).to(device)
    y = torch.stack(y_stack).to(device)
    return x, y

```

---

### 3. ğŸš€ ì˜ˆìƒ ê²°ê³¼

ì´ ìˆ˜ì •ì„ ì ìš©í•˜ë©´:

1. **Multi-sample Noise ì œê±°**: ëª¨ë¸ì€ ì´ì œ ì˜¤ì§ "ê³„ì‚° ê²°ê³¼"ì— ëŒ€í•´ì„œë§Œ Lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ "ë‹¤ìŒ ë¬¸ì œ ì§ˆë¬¸"ì€ ë¬´ì‹œ(`-1`)ë©ë‹ˆë‹¤.
2. **Loss ê¸‰ë½**: í˜„ì¬ 1.24ì—ì„œ ë©ˆì¶˜ Lossê°€ ìˆœì‹ê°„ì— **0.1 ë¯¸ë§Œ**ìœ¼ë¡œ ë–¨ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤.
3. **Grokking ì‹œì‘**: Train Lossê°€ 0ì— ê°€ê¹Œì›Œì ¸ì•¼ ë¹„ë¡œì†Œ ëª¨ë¸ì´ "íŒ¨í„´"ì„ ì°¾ê¸° ì‹œì‘í•©ë‹ˆë‹¤.



(env) linux@localhost:~/taewony/SPAK/examples/KernelEngineer/looplm$ python run_experiments.py
ğŸš€ Starting 3 Experiments for RoPE & Batching Validation...

============================================================
â–¶ï¸  Running [1/3]: Exp1_Baseline_RoPE_Fixed
    Script: train_baseline_12l.py
    Output: experiments/Exp1_Baseline_RoPE_Fixed
============================================================

[CMD] python train_baseline_12l.py --dataset=addition_reverse --n_layer=12 --n_embd=256 --n_head=4 --max_iters=15000 --batch_size=128 --weight_decay=1e-4 --out_dir=experiments/Exp1_Baseline_RoPE_Fixed
Overriding: dataset = addition_reverse
Overriding: n_layer = 12
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: max_iters = 15000
Overriding: batch_size = 128
Overriding: weight_decay = 0.0001
Overriding: out_dir = experiments/Exp1_Baseline_RoPE_Fixed
Indexing newlines for aligned batching...
Starting 12L Baseline Training on addition_reverse...
step 0: train loss 2.6833, val loss 2.6826, lr 0.0000e+00
iter 0: loss 2.6838, time 12897.58ms
iter 100: loss 2.0401, time 40.63ms
iter 200: loss 1.9176, time 39.95ms
iter 300: loss 1.7037, time 75.75ms
iter 400: loss 1.4741, time 39.81ms
step 500: train loss 1.2719, val loss 1.2710, lr 9.9840e-04
iter 500: loss 1.3017, time 147.69ms
iter 600: loss 1.2930, time 39.81ms
iter 700: loss 1.2674, time 39.87ms
iter 800: loss 1.2817, time 75.57ms
iter 900: loss 1.2625, time 39.81ms
step 1000: train loss 1.2539, val loss 1.2543, lr 9.9192e-04
iter 1000: loss 1.2697, time 150.21ms
iter 1100: loss 1.2785, time 40.31ms
iter 1200: loss 1.2609, time 40.70ms
iter 1300: loss 1.2680, time 40.46ms
iter 1400: loss 1.2486, time 76.42ms
step 1500: train loss 1.2555, val loss 1.2564, lr 9.8054e-04
iter 1500: loss 1.2613, time 184.94ms
iter 1600: loss 1.2516, time 49.51ms
iter 1700: loss 1.2602, time 47.60ms
iter 1800: loss 1.2565, time 82.95ms
iter 1900: loss 1.2568, time 46.63ms
step 2000: train loss 1.2488, val loss 1.2490, lr 9.6437e-04
iter 2000: loss 1.2537, time 172.47ms
iter 2100: loss 1.2426, time 47.72ms
iter 2200: loss 1.2515, time 86.68ms
iter 2300: loss 1.2500, time 48.53ms
iter 2400: loss 1.2464, time 47.81ms
step 2500: train loss 1.2481, val loss 1.2481, lr 9.4360e-04
iter 2500: loss 1.2530, time 158.44ms
iter 2600: loss 1.2494, time 76.26ms
iter 2700: loss 1.2472, time 40.55ms
iter 2800: loss 1.2482, time 40.83ms
iter 2900: loss 1.2421, time 41.01ms
step 3000: train loss 1.2496, val loss 1.2505, lr 9.1847e-04
iter 3000: loss 1.2467, time 147.08ms
iter 3100: loss 1.2549, time 39.87ms
iter 3200: loss 1.2484, time 75.97ms
iter 3300: loss 1.2536, time 39.65ms
iter 3400: loss 1.2581, time 39.90ms
step 3500: train loss 1.2481, val loss 1.2485, lr 8.8924e-04
iter 3500: loss 1.2568, time 146.79ms
iter 3600: loss 1.2506, time 39.64ms
iter 3700: loss 1.2521, time 75.44ms
iter 3800: loss 1.2441, time 39.43ms
iter 3900: loss 1.2452, time 39.27ms
step 4000: train loss 1.2475, val loss 1.2478, lr 8.5624e-04
iter 4000: loss 1.2567, time 144.86ms
iter 4100: loss 1.2621, time 39.27ms
iter 4200: loss 1.2455, time 39.64ms
iter 4300: loss 1.2486, time 76.23ms
iter 4400: loss 1.2630, time 40.32ms
step 4500: train loss 1.2479, val loss 1.2480, lr 8.1985e-04
iter 4500: loss 1.2587, time 149.39ms
iter 4600: loss 1.2432, time 40.19ms
iter 4700: loss 1.2421, time 44.17ms
iter 4800: loss 1.2576, time 83.95ms
iter 4900: loss 1.2522, time 49.15ms
step 5000: train loss 1.2472, val loss 1.2487, lr 7.8046e-04
iter 5000: loss 1.2327, time 169.24ms
iter 5100: loss 1.2510, time 39.37ms
iter 5200: loss 1.2444, time 75.48ms
iter 5300: loss 1.2473, time 39.48ms
iter 5400: loss 1.2535, time 39.45ms
step 5500: train loss 1.2467, val loss 1.2472, lr 7.3850e-04
iter 5500: loss 1.2477, time 145.29ms
iter 5600: loss 1.2546, time 39.49ms
iter 5700: loss 1.2542, time 39.34ms
iter 5800: loss 1.2534, time 75.07ms
iter 5900: loss 1.2504, time 39.30ms
step 6000: train loss 1.2462, val loss 1.2487, lr 6.9446e-04
iter 6000: loss 1.2432, time 149.69ms
iter 6100: loss 1.2432, time 40.45ms
iter 6200: loss 1.2515, time 40.48ms
iter 6300: loss 1.2521, time 76.73ms
iter 6400: loss 1.2441, time 47.34ms
step 6500: train loss 1.2462, val loss 1.2475, lr 6.4881e-04
iter 6500: loss 1.2466, time 194.01ms
iter 6600: loss 1.2479, time 47.99ms
iter 6700: loss 1.2451, time 82.16ms
iter 6800: loss 1.2387, time 47.48ms
iter 6900: loss 1.2428, time 46.28ms
step 7000: train loss 1.2461, val loss 1.2481, lr 6.0207e-04
iter 7000: loss 1.2427, time 168.24ms
iter 7100: loss 1.2512, time 84.05ms
iter 7200: loss 1.2426, time 51.22ms
iter 7300: loss 1.2402, time 51.21ms
iter 7400: loss 1.2454, time 50.76ms
step 7500: train loss 1.2461, val loss 1.2472, lr 5.5474e-04
iter 7500: loss 1.2389, time 189.05ms
iter 7600: loss 1.2510, time 40.17ms
iter 7700: loss 1.2485, time 39.90ms
iter 7800: loss 1.2343, time 40.03ms
iter 7900: loss 1.2585, time 39.70ms
step 8000: train loss 1.2462, val loss 1.2493, lr 5.0737e-04
iter 8000: loss 1.2332, time 146.17ms
iter 8100: loss 1.2374, time 75.38ms
iter 8200: loss 1.2486, time 39.60ms
iter 8300: loss 1.2443, time 39.61ms
iter 8400: loss 1.2550, time 39.61ms
step 8500: train loss 1.2452, val loss 1.2476, lr 4.6047e-04
iter 8500: loss 1.2428, time 146.18ms
iter 8600: loss 1.2592, time 75.42ms
iter 8700: loss 1.2466, time 39.65ms
iter 8800: loss 1.2444, time 39.29ms
iter 8900: loss 1.2521, time 39.38ms
step 9000: train loss 1.2450, val loss 1.2479, lr 4.1456e-04
iter 9000: loss 1.2510, time 144.75ms
iter 9100: loss 1.2480, time 39.28ms
iter 9200: loss 1.2391, time 75.71ms
iter 9300: loss 1.2618, time 39.96ms
iter 9400: loss 1.2481, time 39.89ms
step 9500: train loss 1.2443, val loss 1.2482, lr 3.7015e-04
iter 9500: loss 1.2597, time 148.31ms
iter 9600: loss 1.2483, time 39.91ms
iter 9700: loss 1.2463, time 83.79ms
iter 9800: loss 1.2405, time 47.36ms
iter 9900: loss 1.2387, time 49.04ms
step 10000: train loss 1.2451, val loss 1.2484, lr 3.2774e-04
iter 10000: loss 1.2431, time 162.44ms
iter 10100: loss 1.2364, time 39.43ms
iter 10200: loss 1.2489, time 75.42ms
iter 10300: loss 1.2441, time 39.44ms
iter 10400: loss 1.2335, time 39.35ms
step 10500: train loss 1.2431, val loss 1.2494, lr 2.8780e-04
iter 10500: loss 1.2493, time 146.25ms
iter 10600: loss 1.2509, time 39.37ms
iter 10700: loss 1.2506, time 75.18ms
iter 10800: loss 1.2446, time 39.36ms
iter 10900: loss 1.2467, time 39.41ms
step 11000: train loss 1.2429, val loss 1.2486, lr 2.5078e-04
iter 11000: loss 1.2379, time 148.64ms
iter 11100: loss 1.2399, time 40.06ms
iter 11200: loss 1.2416, time 75.78ms
iter 11300: loss 1.2565, time 40.14ms
iter 11400: loss 1.2415, time 44.83ms
step 11500: train loss 1.2420, val loss 1.2494, lr 2.1707e-04
iter 11500: loss 1.2421, time 195.42ms
iter 11600: loss 1.2442, time 83.47ms
iter 11700: loss 1.2393, time 45.51ms
iter 11800: loss 1.2484, time 48.35ms
iter 11900: loss 1.2429, time 46.75ms
step 12000: train loss 1.2416, val loss 1.2499, lr 1.8706e-04
