python run_experiments.py
üöÄ Starting 3 Experiments for RoPE & Batching Validation...

============================================================
‚ñ∂Ô∏è  Running [1/3]: Exp1_Baseline_RoPE_Fixed
    Script: train_baseline_12l.py
    Output: experiments/Exp1_Baseline_RoPE_Fixed
============================================================

[CMD] python train_baseline_12l.py --dataset=addition_reverse --n_layer=12 --n_embd=256 --n_head=4 --max_iters=15000 --batch_size=128 --weight_decay=1e-4 --out_dir=experiments/Exp1_Baseline_RoPE_Fixed
Overriding: dataset = addition_reverse
Overriding: n_layer = 12
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: max_iters = 15000
Overriding: batch_size = 128
Overriding: weight_decay = 0.0001
Overriding: out_dir = experiments/Exp1_Baseline_RoPE_Fixed
Indexing newlines for aligned batching...
Starting 12L Baseline Training on addition_reverse...
step 0: train loss 2.6833, val loss 2.6826, lr 0.0000e+00
iter 0: loss 2.6838, time 12897.58ms
iter 100: loss 2.0401, time 40.63ms
iter 200: loss 1.9176, time 39.95ms
iter 300: loss 1.7037, time 75.75ms
iter 400: loss 1.4741, time 39.81ms
step 500: train loss 1.2719, val loss 1.2710, lr 9.9840e-04
iter 500: loss 1.3017, time 147.69ms
iter 600: loss 1.2930, time 39.81ms
iter 700: loss 1.2674, time 39.87ms
iter 800: loss 1.2817, time 75.57ms
iter 900: loss 1.2625, time 39.81ms
step 1000: train loss 1.2539, val loss 1.2543, lr 9.9192e-04
iter 1000: loss 1.2697, time 150.21ms
iter 1100: loss 1.2785, time 40.31ms
iter 1200: loss 1.2609, time 40.70ms
iter 1300: loss 1.2680, time 40.46ms
iter 1400: loss 1.2486, time 76.42ms
step 1500: train loss 1.2555, val loss 1.2564, lr 9.8054e-04
iter 1500: loss 1.2613, time 184.94ms
iter 1600: loss 1.2516, time 49.51ms
iter 1700: loss 1.2602, time 47.60ms
iter 1800: loss 1.2565, time 82.95ms
iter 1900: loss 1.2568, time 46.63ms
step 2000: train loss 1.2488, val loss 1.2490, lr 9.6437e-04
iter 2000: loss 1.2537, time 172.47ms
iter 2100: loss 1.2426, time 47.72ms
iter 2200: loss 1.2515, time 86.68ms
iter 2300: loss 1.2500, time 48.53ms
iter 2400: loss 1.2464, time 47.81ms
step 2500: train loss 1.2481, val loss 1.2481, lr 9.4360e-04
iter 2500: loss 1.2530, time 158.44ms
iter 2600: loss 1.2494, time 76.26ms
