python run_experiments.py
üöÄ Starting 3 Experiments for RoPE & Batching Validation...

============================================================
‚ñ∂Ô∏è  Running [1/3]: Exp1_Baseline_RoPE_Fixed
    Script: train_baseline_12l.py
    Output: experiments/Exp1_Baseline_RoPE_Fixed
============================================================

[CMD] python train_baseline_12l.py --dataset=addition_reverse --n_layer=12 --n_embd=256 --n_head=4 --max_iters=15000 --batch_size=128 --weight_decay=1e-4 --out_dir=experiments/Exp1_Baseline_RoPE_Fixed
Overriding: dataset = addition_reverse
Overriding: n_layer = 12
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: max_iters = 15000
Overriding: batch_size = 128
Traceback (most recent call last):
  File "/home/linux/taewony/SPAK/examples/KernelEngineer/looplm/train_baseline_12l.py", line 47, in <module>
    exec(open(_configurator_path).read())
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 47, in <module>
ValueError: Unknown config key: weight_decay
‚ùå Experiment Exp1_Baseline_RoPE_Fixed failed with return code 1

============================================================
‚ñ∂Ô∏è  Running [2/3]: Exp2_LoopLM_RoPE_Fixed
    Script: train_loop.py
    Output: experiments/Exp2_LoopLM_RoPE_Fixed
============================================================

[CMD] python train_loop.py --dataset=addition_reverse --num_loops=12 --n_embd=256 --n_head=4 --inject_x0=False --max_iters=15000 --batch_size=128 --weight_decay=1e-4 --out_dir=experiments/Exp2_LoopLM_RoPE_Fixed
Overriding: dataset = addition_reverse
Overriding: num_loops = 12
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: inject_x0 = False
Overriding: max_iters = 15000
Overriding: batch_size = 128
Traceback (most recent call last):
  File "/home/linux/taewony/SPAK/examples/KernelEngineer/looplm/train_loop.py", line 41, in <module>
    exec(open(_configurator_path).read())
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 47, in <module>
ValueError: Unknown config key: weight_decay
‚ùå Experiment Exp2_LoopLM_RoPE_Fixed failed with return code 1

============================================================
‚ñ∂Ô∏è  Running [3/3]: Exp3_LoopLM_Ultimate_Thinking
    Script: train_loop.py
    Output: experiments/Exp3_LoopLM_Ultimate_Thinking
============================================================

[CMD] python train_loop.py --dataset=addition_reverse --num_loops=30 --n_embd=256 --n_head=4 --inject_x0=False --max_iters=20000 --batch_size=128 --dropout=0.1 --out_dir=experiments/Exp3_LoopLM_Ultimate_Thinking
Overriding: dataset = addition_reverse
Overriding: num_loops = 30
Overriding: n_embd = 256
Overriding: n_head = 4
Overriding: inject_x0 = False
Overriding: max_iters = 20000
Overriding: batch_size = 128
Overriding: dropout = 0.1
Overriding: out_dir = experiments/Exp3_LoopLM_Ultimate_Thinking
Loading data from: /home/linux/taewony/SPAK/examples/KernelEngineer/looplm/data/addition_reverse
Indexing newlines for aligned batching...
No previous checkpoint found. Starting from scratch.
Starting LoopLM Training on addition_reverse...
Config: 30 loops over 1 layer block
step 0: train loss 2.5977, val loss 2.5975, lr 0.0000e+00
iter 0: loss 2.5929, time 20910.61ms
iter 100: loss 2.0527, time 92.01ms
iter 200: loss 2.0107, time 127.49ms
iter 300: loss 1.9648, time 91.22ms
iter 400: loss 1.9520, time 91.11ms
step 500: train loss 1.9300, val loss 1.9308, lr 9.9840e-04
iter 500: loss 1.9010, time 283.04ms
iter 600: loss 1.8646, time 93.55ms
iter 700: loss 1.7496, time 106.93ms
iter 800: loss 1.6708, time 153.11ms
iter 900: loss 1.4570, time 98.77ms
step 1000: train loss 1.3824, val loss 1.3817, lr 9.9192e-04
iter 1000: loss 1.3478, time 280.47ms
iter 1100: loss 1.3226, time 91.95ms
iter 1200: loss 1.2866, time 92.33ms
iter 1300: loss 1.2727, time 128.30ms
iter 1400: loss 1.2807, time 93.32ms
step 1500: train loss 1.3342, val loss 1.3349, lr 9.8054e-04
iter 1500: loss 1.2679, time 323.21ms
iter 1600: loss 1.2621, time 120.28ms
iter 1700: loss 1.2608, time 142.18ms
iter 1800: loss 1.2685, time 105.87ms
iter 1900: loss 1.2609, time 109.60ms
step 2000: train loss 1.3332, val loss 1.3332, lr 9.6437e-04
iter 2000: loss 1.2644, time 334.10ms
iter 2100: loss 1.2467, time 128.41ms
