ì œê³µëœ `AttentionFMHA.py`ëŠ” NVIDIA cuTile ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ **Fused Multi-Head Attention (FMHA)** ë¥¼ GPUì—ì„œ ê³ íš¨ìœ¨ë¡œ ì‹¤í–‰í•˜ëŠ” ì»¤ë„ ì½”ë“œì…ë‹ˆë‹¤. í•µì‹¬ì€ **íƒ€ì¼ë§(Tiling)**ê³¼ **Online Softmax** ê¸°ë²•ì„ í†µí•´, ëŒ€ê·œëª¨ Attention ê³„ì‚°ì„ GPU ë©”ëª¨ë¦¬ ê³„ì¸µ(L1/L2 ìºì‹œ, ê³µìœ  ë©”ëª¨ë¦¬, ë ˆì§€ìŠ¤í„°)ì— ìµœì í™”í•˜ì—¬ ì—°ì‚°í•˜ëŠ” ë° ìˆìŠµë‹ˆë‹¤.

ì£¼ìš” ìµœì í™” ê¸°ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

* **Fusion**: QK ì—°ì‚°, Softmax, PV ì—°ì‚°ì„ í•˜ë‚˜ì˜ ì»¤ë„ë¡œ ìœµí•©í•˜ì—¬ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì „ì—­ ë©”ëª¨ë¦¬ì— ì“°ê³  ì½ëŠ” ë¹„ìš©ì„ ì œê±°í•©ë‹ˆë‹¤.  
* **Online Softmax**: ì‹œí€€ìŠ¤ ì „ì²´(`N`)ë¥¼ í•œ ë²ˆì— Softmaxí•˜ì§€ ì•Šê³ , í‚¤/ë°¸ë¥˜ íƒ€ì¼(`TILE_N`)ì„ ìˆœíšŒí•˜ë©° ëˆ„ì ëœ í†µê³„ëŸ‰(`m_i`, `l_i`)ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.  
* **Tiling**: í° í–‰ë ¬ ì—°ì‚°ì„ `TILE_M`(ì¿¼ë¦¬), `TILE_N`(í‚¤/ë°¸ë¥˜), `TILE_D`(í—¤ë“œ ì°¨ì›) í¬ê¸°ì˜ ì‘ì€ íƒ€ì¼ë¡œ ë¶„í•´í•˜ì—¬ ë ˆì§€ìŠ¤í„°/ê³µìœ  ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.

ì•„ë˜ëŠ” ìš”ì²­í•˜ì‹  **ì„¸ ê°€ì§€ Attention êµ¬í˜„ (ë‹¨ìˆœ, Fused, Fused Multi-Head)** ì„ `einsum` ìˆ˜ì‹ê³¼ DSL ì½”ë“œë¡œ ì—­ê³µí•™í•˜ì—¬ ì‘ì„±í•œ ë¹„êµì…ë‹ˆë‹¤.

---

### **1\. ë‹¨ìˆœ Attention Mechanism (Naive)**

ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœë¡œ, ìˆ˜í•™ì  ì •ì˜ì™€ ë…¼ë¦¬ì  íë¦„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

**ìˆ˜í•™ì‹ (Einsum)**: `Output[b, m, :] = softmax( (Q[b, m, :] @ K[b, :, n]^T) / sqrt(d_k) ) @ V[b, n, :]` (ë˜ëŠ” `"bmd,bnd->bmn; bmn,bnd->bmd"`ë¡œ ë¶„ë¦¬ í‘œí˜„)

**DSL ì½”ë“œ**:

\# Procedure: naive\_attention\_kernel

\# Input: Q\[B, M, D\], K\[B, N, D\], V\[B, N, D\]

\# Output: O\[B, M, D\]

\# Einsum: O\[b,m,d\] \= Î£\_n Softmax( Î£\_d Q\[b,m,d\]\*K\[b,n,d\] / âˆšd\_k )\[n\] \* V\[b,n,d\]

procedure naive\_attention\_kernel:

    b \= blockIdx.x  \# ë°°ì¹˜ ì¸ë±ìŠ¤

    m \= blockIdx.y  \# ì¿¼ë¦¬ ìœ„ì¹˜ ì¸ë±ìŠ¤

    \# 1\. QK ì—°ì‚°: \[D\] ì°¨ì› ë‚´ì 

    acc\_qk \= zeros(N) \# \[N\]

    for n in 0..N-1:

        for d in 0..D-1:

            acc\_qk\[n\] \+= Q\[b, m, d\] \* K\[b, n, d\]

        acc\_qk\[n\] \= acc\_qk\[n\] / sqrt(D)

    \# 2\. Global Softmax: N ì „ì²´ì— ëŒ€í•œ ì •ê·œí™” í•„ìš”

    m\_max \= max(acc\_qk\[:\])   \# N ì „ì²´ ìµœëŒ€ê°’ íƒìƒ‰

    exp\_sum \= 0

    for n in 0..N-1:

        acc\_qk\[n\] \= exp(acc\_qk\[n\] \- m\_max)

        exp\_sum \+= acc\_qk\[n\]

    for n in 0..N-1:

        attn\[n\] \= acc\_qk\[n\] / exp\_sum \# Softmax ì™„ë£Œ

    \# 3\. PV ì—°ì‚°: Attention ê°€ì¤‘ì¹˜ ì ìš©

    for d in 0..D-1:

        acc\_out\[d\] \= 0

        for n in 0..N-1:

            acc\_out\[d\] \+= attn\[n\] \* V\[b, n, d\]

        O\[b, m, d\] \= acc\_out\[d\]

**ğŸ’¡ íŠ¹ì§•**: `QK`, `Softmax`, `PV` ë‹¨ê³„ê°€ ëª…í™•íˆ ë¶„ë¦¬ë˜ê³ , Softmaxë¥¼ ìœ„í•´ **ì „ì²´ N ì°¨ì›ì— ëŒ€í•œ ìµœëŒ€ê°’(`m_max`)ê³¼ í•©(`exp_sum`)ì„ ë¨¼ì € ê³„ì‚°**í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì¶”ê°€ì ì¸ ì „ì—­ ë©”ëª¨ë¦¬ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.

---

### **2\. Online Softmaxê°€ ì ìš©ëœ Fused Attention**

N ì°¨ì›ì„ íƒ€ì¼ë¡œ ë‚˜ëˆ„ì–´ ìˆœíšŒí•˜ë©°, Softmax í†µê³„ëŸ‰ì„ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì¤‘ê°„ ì €ì¥ì„ í”¼í•©ë‹ˆë‹¤.

**ìˆ˜í•™ì‹ (Einsum)**: `Output[b, m, :] = OnlineSoftmax( (Q[b, m, :] @ K[b, :, n_tile]^T) / sqrt(d_k) ) @ V[b, n_tile, :]`

**DSL ì½”ë“œ**:

\# Procedure: fused\_attention\_online\_softmax\_kernel

\# Input: Q\[B, M, D\], K\[B, N, D\], V\[B, N, D\]

\# Output: O\[B, M, D\]

\# Einsum: O\[b,m,d\] \= Î£\_n\_tile OnlineSoftmax\_Tile( Q\[b,m,d\]\*K\[b,n\_tile,d\] / âˆšd\_k ) \* V\[b,n\_tile,d\]

procedure fused\_attention\_online\_softmax\_kernel:

    b \= blockIdx.x  \# ë°°ì¹˜

    m\_tile \= blockIdx.y \# ì¿¼ë¦¬ íƒ€ì¼ (TILE\_M)

    \# Online Softmax ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”

    m\_i \= \-inf  \# í˜„ì¬ê¹Œì§€ ì²˜ë¦¬í•œ íƒ€ì¼ ì¤‘ ìµœëŒ€ê°’

    l\_i \= 0.0   \# í˜„ì¬ê¹Œì§€ì˜ ì •ê·œí™” í•©

    acc \= zeros(D) \# ì¶œë ¥ ëˆ„ì ê¸°

    \# í‚¤/ë°¸ë¥˜ íƒ€ì¼ ìˆœíšŒ (N ì°¨ì›ì„ TILE\_N í¬ê¸°ë¡œ ë‚˜ëˆ”)

    for j in 0..(N/TILE\_N)-1:

        \# 1\. QK íƒ€ì¼ ì—°ì‚°

        q\_tile \= load(Q\[b, m\_tile, 0:D\]) \# \[TILE\_M, D\]

        k\_tile \= load(K\[b, j\*TILE\_N : (j+1)\*TILE\_N, 0:D\]) \# \[TILE\_N, D\]

        qk\_tile \= matmul(q\_tile, k\_tile.T) / sqrt(D) \# \[TILE\_M, TILE\_N\]

        \# 2\. Online Softmax ì—…ë°ì´íŠ¸ (í˜„ì¬ íƒ€ì¼ì— ëŒ€í•´ì„œë§Œ)

        m\_ij \= max(m\_i, max(qk\_tile, dim=-1)) \# \[TILE\_M, 1\]

        p\_tile \= exp(qk\_tile \- m\_ij)          \# \[TILE\_M, TILE\_N\]

        l\_ij \= sum(p\_tile, dim=-1)            \# \[TILE\_M, 1\]

        \# 3\. ì´ì „ ëˆ„ì ê°’(acc)ê³¼ í˜„ì¬ í†µê³„ëŸ‰ì„ ì¡°ì •

        alpha \= exp(m\_i \- m\_ij)

        l\_i \= l\_i \* alpha \+ l\_ij

        acc \= acc \* alpha \# ì¶œë ¥ ëˆ„ì ê°’ ì¡°ì •

        \# 4\. PV íƒ€ì¼ ì—°ì‚° ë° ëˆ„ì 

        v\_tile \= load(V\[b, j\*TILE\_N : (j+1)\*TILE\_N, 0:D\]) \# \[TILE\_N, D\]

        acc \= acc \+ matmul(p\_tile, v\_tile) \# \[TILE\_M, D\]

        m\_i \= m\_ij \# ìƒíƒœ ì—…ë°ì´íŠ¸

    \# 5\. ìµœì¢… ì •ê·œí™” ë° ì €ì¥

    O\[b, m\_tile, 0:D\] \= acc / l\_i

**ğŸ’¡ íŠ¹ì§•**: **`m_i`, `l_i`ë¼ëŠ” ìƒíƒœ ë³€ìˆ˜**ë¥¼ ìœ ì§€í•˜ë©° N ì°¨ì›ì„ íƒ€ì¼(`TILE_N`) ë‹¨ìœ„ë¡œ ìˆœíšŒí•©ë‹ˆë‹¤. ê° íƒ€ì¼ ì²˜ë¦¬ í›„ ëˆ„ì  ì¶œë ¥(`acc`)ê³¼ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬, **ì‹œí€€ìŠ¤ ì „ì²´ì— ëŒ€í•œ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì „ì—­ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**.

---

### **3\. Fused Multi-Head Attention (FMHA)**

Fused Attentionì„ **ë‹¤ì¤‘ í—¤ë“œ(Batch, Head)** ì°¨ì›ìœ¼ë¡œ í™•ì¥í•˜ê³ , GQA(Grouped Query Attention)ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

**ìˆ˜í•™ì‹ (Einsum)**: `Output[b, h, m, :] = OnlineSoftmax( (Q[b, h, m, :] @ K[b, h//G, :, :]^T) / sqrt(d_k) ) @ V[b, h//G, :, :]` (ì—¬ê¸°ì„œ `G`ëŠ” `QUERY_GROUP_SIZE`)

**DSL ì½”ë“œ**:

\# Procedure: fused\_multihead\_attention\_kernel

\# Input: Q\[B, H, M, D\], K\[B, H/KV\_Heads, N, D\], V\[B, H/KV\_Heads, N, D\]

\# Output: O\[B, H, M, D\]

\# Einsum: O\[b,h,m,d\] \= Î£\_n\_tile OnlineSoftmax\_Tile( Q\[b,h,m,d\]\*K\[b,g,n\_tile,d\] / âˆšd\_k ) \* V\[b,g,n\_tile,d\]

\# where g \= h // QUERY\_GROUP\_SIZE

procedure fused\_multihead\_attention\_kernel:

    \# 3D ê·¸ë¦¬ë“œ ë§¤í•‘: (ì¿¼ë¦¬íƒ€ì¼, ë°°ì¹˜\*í—¤ë“œ, 1\)

    bid\_x \= blockIdx.x \# ì²˜ë¦¬í•  ì¿¼ë¦¬ ì‹œí€€ìŠ¤ íƒ€ì¼ (M ì°¨ì›)

    bid\_y \= blockIdx.y \# ë°°ì¹˜ì™€ í—¤ë“œë¥¼ ê²°í•©í•œ ì¸ë±ìŠ¤

    \# ë°°ì¹˜(b)ì™€ í—¤ë“œ(h) ì¸ë±ìŠ¤ ë””ì½”ë”©

    batch\_idx \= bid\_y // H

    head\_idx \= bid\_y % H

    kv\_head\_idx \= head\_idx // QUERY\_GROUP\_SIZE \# GQAë¥¼ ìœ„í•œ KV í—¤ë“œ ì¸ë±ìŠ¤

    \# Online Softmax ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™” (íƒ€ì¼ ë‹¨ìœ„)

    m\_i \= full((TILE\_M, 1), \-inf)

    l\_i \= full((TILE\_M, 1), 0.0)

    acc \= full((TILE\_M, D), 0.0)

    \# í˜„ì¬ ë¸”ë¡ì´ ë‹´ë‹¹í•˜ëŠ” ì¿¼ë¦¬ íƒ€ì¼ ë¡œë“œ \[TILE\_M, D\]

    q\_tile \= load(Q\[batch\_idx, head\_idx, bid\_x\*TILE\_M : , :\])

    \# í‚¤/ë°¸ë¥˜ íƒ€ì¼ ìˆœíšŒ (N ì°¨ì›)

    for j in 0..(N/TILE\_N)-1:

        \# 1\. QK íƒ€ì¼ ì—°ì‚°: ì¿¼ë¦¬ í—¤ë“œì— ëŒ€ì‘í•˜ëŠ” KV í—¤ë“œ ì‚¬ìš©

        k\_tile \= load(K\[batch\_idx, kv\_head\_idx, j\*TILE\_N : , :\]) \# \[TILE\_N, D\]

        qk\_tile \= matmul(q\_tile, k\_tile.T) \* qk\_scale \# \[TILE\_M, TILE\_N\]

        \# 2\. (ì˜µì…˜) ìºì£¼ì–¼ ë§ˆìŠ¤í‚¹ ì ìš© (causal=True ì‹œ)

        if causal:

            qk\_tile \= apply\_causal\_mask(qk\_tile, bid\_x, j)

        \# 3\. Online Softmax ì—…ë°ì´íŠ¸ (íƒ€ì¼ë³„)

        m\_ij \= max(m\_i, max(qk\_tile, dim=-1, keepdims=True))

        p\_tile \= exp2(qk\_tile \- m\_ij) \# cuTileì€ exp2 ì‚¬ìš©

        l\_ij \= sum(p\_tile, dim=-1, keepdims=True)

        alpha \= exp2(m\_i \- m\_ij)

        l\_i \= l\_i \* alpha \+ l\_ij

        acc \= acc \* alpha

        \# 4\. PV íƒ€ì¼ ì—°ì‚° ë° ëˆ„ì 

        v\_tile \= load(V\[batch\_idx, kv\_head\_idx, j\*TILE\_N : , :\]) \# \[TILE\_N, D\]

        acc \= acc \+ matmul(p\_tile.astype(V.dtype), v\_tile)

        m\_i \= m\_ij

    \# 5\. ìµœì¢… ì •ê·œí™” ë° ì €ì¥

    O\[batch\_idx, head\_idx, bid\_x\*TILE\_M : , :\] \= acc / l\_i

**ğŸ’¡ íŠ¹ì§•**: **ë°°ì¹˜(`b`)ì™€ í—¤ë“œ(`h`) ì°¨ì›ì„ 2D ê·¸ë¦¬ë“œ(`blockIdx.y`)ë¡œ ê²°í•©**í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤. `kv_head_idx` ê³„ì‚°ì„ í†µí•´ **GQA**ë¥¼ ì§€ì›í•˜ë©°, **ìºì£¼ì–¼ ë§ˆìŠ¤í‚¹** ë¡œì§ì´ ì¶”ê°€ë©ë‹ˆë‹¤.

---

### **ğŸ“Š 3ê°€ì§€ êµ¬í˜„ ë°©ì‹ì˜ í•µì‹¬ ì°¨ì´ì  ë¹„êµ**

| íŠ¹ì„± | ë‹¨ìˆœ Attention | Fused Attention (Online Softmax) | Fused Multi-Head Attention (FMHA) |
| :---- | :---- | :---- | :---- |
| **ì—°ì‚° íë¦„** | ë‹¨ê³„ë³„ ë¶„ë¦¬(`QK`â†’`Softmax`â†’`PV`) | **íƒ€ì¼ ê¸°ë°˜ Fusion** (`QK`â†’`Softmax ì—…ë°ì´íŠ¸`â†’`PV` in loop) | **Multi-Head ë³‘ë ¬í™” ì¶”ê°€** |
| **ì†Œí”„íŠ¸ë§¥ìŠ¤** | Global (ì „ì²´ N ëŒ€ìƒ) | **Online (íƒ€ì¼ë³„ ëˆ„ì )** | Online \+ **ë‹¤ì¤‘ í—¤ë“œë³„ ë…ë¦½ ì²˜ë¦¬** |
| **ë©”ëª¨ë¦¬ ì ‘ê·¼** | ì¤‘ê°„ ê²°ê³¼(`qk`, `attn`) ì „ì—­ ë©”ëª¨ë¦¬ ì €ì¥ | ì¤‘ê°„ ê²°ê³¼ **ë ˆì§€ìŠ¤í„°/ê³µìœ  ë©”ëª¨ë¦¬ ìœ ì§€**, ì „ì—­ ë©”ëª¨ë¦¬ ì ‘ê·¼ **ìµœì†Œí™”** | **í—¤ë“œ ê°„ ë³‘ë ¬ ì²˜ë¦¬**ë¡œ ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë„ í–¥ìƒ |
| **ë³‘ë ¬í™” ì¶•** | ì£¼ë¡œ `B`, `M` | `B`, `M` \+ **`N`ì„ íƒ€ì¼ë¡œ ë¶„í• ** | **`B*H`ë¥¼ ê²°í•©í•œ 2D ê·¸ë¦¬ë“œ** \+ `M`, `N` íƒ€ì¼ë§ |
| **ì£¼ìš” ìµœì í™”** | \- | **Fusion, Online Softmax** | **Multi-Head ë³‘ë ¬í™”, GQA, Causal Masking** |
| **cuTile í•¨ìˆ˜ ë§¤í•‘** | \- | `ct.load`, `ct.mma`, `ct.max`, `ct.sum`, `ct.exp2` | ìœ„ í•¨ìˆ˜ë“¤ \+ **ê·¸ë¦¬ë“œ ë§¤í•‘ (`bid()`)**, **ì¡°ê±´ë¶€ ë§ˆìŠ¤í‚¹** |

ì´ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ì œê³µëœ `AttentionFMHA.py` ì½”ë“œëŠ” **3ë²ˆì§¸ Fused Multi-Head Attention (FMHA)** ì„ êµ¬í˜„í•œ ê²ƒì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” GPUì˜ ë©”ëª¨ë¦¬ ê³„ì¸µê³¼ ëŒ€ê·œëª¨ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ ì„¤ê³„ì…ë‹ˆë‹¤.