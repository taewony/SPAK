(env) linux@localhost:~/taewony/SPAK/cutile$ python cutile_matmul_perf.py
=== MatMul Benchmark (4096x4096x4096) ===
Iterations: 100, CTAs: 16
--------------------------------------------------
PyTorch (cuBLAS)          : 2.022 ms
cuTile (No Swizzle)       : 6.718 ms
cuTile (Group=2)          : 6.715 ms
cuTile (Group=4)          : 6.716 ms
--------------------------------------------------
Verifying correctness (based on Group=4 run)...
✅ Verification: Success!
============================================================
Method                    | Time (ms)  | Rel. Speed
------------------------------------------------------------
PyTorch (cuBLAS)          | 2.022 ms  | 1.00x
cuTile (No Swizzle)       | 6.718 ms  | 0.30x
cuTile (Group=2)          | 6.715 ms  | 0.30x
cuTile (Group=4)          | 6.716 ms  | 0.30x
============================================================
>> Gap between Best/Worst cuTile: 0.04%
>> Note: Swizzling is providing performance gain.


(env) linux@localhost:~/taewony/SPAK/examples/KernelEngineer$ python matmul_improved_2.py
>> Detected GPU with 48 SMs. Launching 96 CTAs.
============================================================
=== MatMul Benchmark Optimized (4096x4096x4096) ===
SM Count: 48, Active CTAs: 96
------------------------------------------------------------
PyTorch (cuBLAS)          : 1.993 ms
cuTile (No Swizzle)       : 2.325 ms
cuTile (Group=2)          : 2.333 ms
cuTile (Group=4)          : 2.334 ms
------------------------------------------------------------
Verifying correctness (based on Group=4)...
✅ Verification: Success!
============================================================
Method                    | Time (ms)  | Rel. Speed
------------------------------------------------------------
PyTorch (cuBLAS)          | 1.993 ms  | 1.00x
cuTile (No Swizzle)       | 2.325 ms  | 0.86x
cuTile (Group=2)          | 2.333 ms  | 0.85x
cuTile (Group=4)          | 2.334 ms  | 0.85x
============================================================
>> ℹ️ NOTE: Performance is similar. Bottleneck might still be compute-bound or launch overhead.
(env) linux@localhost:~/taewony/SPAK/examples/KernelEngineer$ python matmul_improved_spak.py
============================================================
=== SPAK Optimized Benchmark (4096x4096x4096) ===
SM Count: 48, Active CTAs: 96
------------------------------------------------------------
PyTorch (cuBLAS)          : 1.999 ms
SPAK (Swizzle+Pipeline)   : 2.936 ms
------------------------------------------------------------
Verifying correctness...
✅ Verification: Success!
============================================================
Method                    | Time (ms)  | Rel. Speed
------------------------------------------------------------
PyTorch (cuBLAS)          | 1.999 ms  | 1.00x
SPAK Optimized              | 2.936 ms   | 0.68x
============================================================
(env) linux@localhost:~/taewony/SPAK/examples/KernelEngineer$


(env) linux@localhost:~/taewony/SPAK/examples/KernelEngineer$ python matmul_autotuned.py
>> SPAK Auto-Tuner on NVIDIA GeForce RTX 5070 | SMs: 48
>> Strategy: Double-Buffered Pipeline + Config Sweep
--------------------------------------------------------------------------------------------------------------
Size       | PyTorch  | SPAK Tuned | Speedup  | Best Config
(MxNxK)    | (TFLOPS) | (TFLOPS)   | (%)          | (Tile_M x Tile_N x Tile_K)
--------------------------------------------------------------------------------------------------------------
2048       | 65.39    | 63.96      | -2.2   % | 64x64x64 (Occ=2)
4096       | 68.70    | 67.32      | -2.0   % | 64x64x64 (Occ=2)
8192       | 67.09    | 65.09      | -3.0   % | 64x64x64 (Occ=2)
--------------------------------------------------------------------------------------------------------------
