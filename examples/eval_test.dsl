system_model EvaluationDemo {
  axiom: "Evaluation must be quantitative."
}

task TaskWithEval {
  step s1: tool.run {
    cmd: "echo 'This is a sample output generated by the agent.'"
    output_var: agent_output
  }

  evaluation {
    check output_length: tool.run {
      # In reality, this would be a python script computing length.
      # Here we simulate it with echo
      cmd: "echo 10" 
      output_var: length_score
    }
    
    check relevance: llm.query {
      # LLM-as-a-Judge
      role: "Evaluator"
      prompt_template: "Rate the relevance of: '{{agent_output}}' on scale 0-1. Output ONLY the number."
      output_var: relevance_score
    }
  }
}
